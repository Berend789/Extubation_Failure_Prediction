{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "3b7e431a861a72513f361682db481bb043ab213b57189a56d42fe6b32fa57c58"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\berend\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\nC:\\Users\\berend\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\nC:\\Users\\berend\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\nC:\\Users\\berend\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from all_stand_var import conv_dict, lab_cols, vent_cols3\n",
    "from all_own_funct import memory_downscale,memory_upscale,evaluate\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "from seaborn import heatmap\n",
    "from sklearn.metrics import roc_curve, accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy.stats import kurtosis\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import keras.backend as K\n",
    "from RNN_LTSM_CHD import return_loaded_model\n",
    "import pickle\n",
    "import locale\n",
    "import LR_build_CHD as pp\n",
    "import tables\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve, accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "locale.setlocale(locale.LC_ALL, 'fr_FR')\n",
    "output_folder = os.path.join(os.getcwd(), 'Results_LR_RF_CHD_v3','1u_Results_no_mis')\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nMultiIndex: 1469448 entries, (1, 7171037, Timestamp('2008-04-11 00:00:00')) to (789, 3536787, Timestamp('2020-01-10 00:00:00'))\nData columns (total 17 columns):\n #   Column           Non-Null Count    Dtype         \n---  ------           --------------    -----         \n 0   pat_datetime     1469448 non-null  datetime64[ns]\n 1   pat_weight_act   1469448 non-null  float16       \n 2   mon_etco2        1457208 non-null  float16       \n 3   mon_hr           1469448 non-null  float16       \n 4   mon_ibp_mean     1469448 non-null  float16       \n 5   mon_rr           1469448 non-null  float16       \n 6   mon_sat          1469448 non-null  float16       \n 7   vent_m_fio2      1469448 non-null  float16       \n 8   vent_m_peep      1458648 non-null  float16       \n 9   vent_m_ppeak     1458648 non-null  float16       \n 10  vent_m_rr        1458648 non-null  float16       \n 11  vent_m_tv_exp    1458648 non-null  float16       \n 12  Age              1469448 non-null  int64         \n 13  Reintub          1469448 non-null  float16       \n 14  Extubation_date  1469448 non-null  datetime64[ns]\n 15  Diagnose         1469448 non-null  int64         \n 16  Adnum            1469448 non-null  int64         \ndtypes: datetime64[ns](2), float16(12), int64(3)\nmemory usage: 98.3 MB\nNone\n"
     ]
    }
   ],
   "source": [
    "import tables\n",
    "\n",
    "\n",
    "df=pd.read_hdf(os.path.join('./Results_CHD/', 'processed_df.h5'),key='df',mode='r')\n",
    "#df=memory_downscale(df)\n",
    "print(df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1469448 entries, 0 to 1469447\n",
      "Data columns (total 20 columns):\n",
      " #   Column           Non-Null Count    Dtype         \n",
      "---  ------           --------------    -----         \n",
      " 0   level_0          1469448 non-null  int64         \n",
      " 1   pat_hosp_id      1469448 non-null  int64         \n",
      " 2   OK_datum         1469448 non-null  datetime64[ns]\n",
      " 3   pat_datetime     1469448 non-null  datetime64[ns]\n",
      " 4   pat_weight_act   1469448 non-null  float16       \n",
      " 5   mon_etco2        1457208 non-null  float16       \n",
      " 6   mon_hr           1469448 non-null  float16       \n",
      " 7   mon_ibp_mean     1469448 non-null  float16       \n",
      " 8   mon_rr           1469448 non-null  float16       \n",
      " 9   mon_sat          1469448 non-null  float16       \n",
      " 10  vent_m_fio2      1469448 non-null  float16       \n",
      " 11  vent_m_peep      1458648 non-null  float16       \n",
      " 12  vent_m_ppeak     1458648 non-null  float16       \n",
      " 13  vent_m_rr        1458648 non-null  float16       \n",
      " 14  vent_m_tv_exp    1458648 non-null  float16       \n",
      " 15  Age              1469448 non-null  int64         \n",
      " 16  Reintub          1469448 non-null  float16       \n",
      " 17  Extubation_date  1469448 non-null  datetime64[ns]\n",
      " 18  Diagnose         1469448 non-null  int64         \n",
      " 19  Adnum            1469448 non-null  int64         \n",
      "dtypes: datetime64[ns](3), float16(12), int64(5)\n",
      "memory usage: 123.3 MB\n",
      "['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "df=df.reset_index(drop=False)\n",
    "df.info()\n",
    "def index_1hr(group):\n",
    "    if len(group) > 720:\n",
    "        group=group.sort_values('pat_datetime')\n",
    "        group=group.tail(720)\n",
    "    group['tim']=pd.date_range(start='1/1/2018', periods=len(group), freq='1min')\n",
    "    grouped = pd.Grouper(key='tim', freq='1H')\n",
    "    group['idx_1hr']=group.groupby(grouped,sort=False).ngroup().add(1)\n",
    "    #group['mis']=(group['pat_datetime'].max()-group['pat_datetime'].min()).total_seconds()/60\n",
    "    del group['tim']\n",
    "    return group\n",
    "#grouped = pd.Grouper(key='pat_datetime', freq='60min',label='left',convention='start')\n",
    "#df['idx_1hr']=df.groupby(['Adnum','pat_hosp_id','AdmissionDate','DischargeDate'],sort=False,as_index=False).ngroup().add(1)\n",
    "df=df.groupby('Adnum',sort=False,as_index=False).apply(index_1hr)\n",
    "df['idx_1hr']=df['idx_1hr'].astype(str)\n",
    "print(df['idx_1hr'].unique())\n",
    "#df[['mis']]=StandardScaler().fit_transform(df[['mis']])\n",
    "df=df.set_index(['pat_hosp_id','OK_datum'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nMultiIndex: 1030725 entries, (7171037, Timestamp('2008-04-11 00:00:00')) to (3536787, Timestamp('2020-01-10 00:00:00'))\nData columns (total 19 columns):\n #   Column           Non-Null Count    Dtype         \n---  ------           --------------    -----         \n 0   level_0          1030725 non-null  int64         \n 1   pat_datetime     1030725 non-null  datetime64[ns]\n 2   pat_weight_act   1030725 non-null  float16       \n 3   mon_etco2        1022085 non-null  float16       \n 4   mon_hr           1030725 non-null  float16       \n 5   mon_ibp_mean     1030725 non-null  float16       \n 6   mon_rr           1030725 non-null  float16       \n 7   mon_sat          1030725 non-null  float16       \n 8   vent_m_fio2      1030725 non-null  float16       \n 9   vent_m_peep      1022805 non-null  float16       \n 10  vent_m_ppeak     1022805 non-null  float16       \n 11  vent_m_rr        1022805 non-null  float16       \n 12  vent_m_tv_exp    1022805 non-null  float16       \n 13  Age              1030725 non-null  int64         \n 14  Reintub          1030725 non-null  float16       \n 15  Extubation_date  1030725 non-null  datetime64[ns]\n 16  Diagnose         1030725 non-null  int64         \n 17  Adnum            1030725 non-null  int64         \n 18  idx_1hr          1030725 non-null  object        \ndtypes: datetime64[ns](2), float16(12), int64(4), object(1)\nmemory usage: 82.7+ MB\nNone\n<class 'pandas.core.frame.DataFrame'>\nMultiIndex: 292706 entries, (9596291, Timestamp('2008-04-24 00:00:00')) to (3292457, Timestamp('2019-11-11 00:00:00'))\nData columns (total 19 columns):\n #   Column           Non-Null Count   Dtype         \n---  ------           --------------   -----         \n 0   level_0          292706 non-null  int64         \n 1   pat_datetime     292706 non-null  datetime64[ns]\n 2   pat_weight_act   292706 non-null  float16       \n 3   mon_etco2        289826 non-null  float16       \n 4   mon_hr           292706 non-null  float16       \n 5   mon_ibp_mean     292706 non-null  float16       \n 6   mon_rr           292706 non-null  float16       \n 7   mon_sat          292706 non-null  float16       \n 8   vent_m_fio2      292706 non-null  float16       \n 9   vent_m_peep      290546 non-null  float16       \n 10  vent_m_ppeak     290546 non-null  float16       \n 11  vent_m_rr        290546 non-null  float16       \n 12  vent_m_tv_exp    290546 non-null  float16       \n 13  Age              292706 non-null  int64         \n 14  Reintub          292706 non-null  float16       \n 15  Extubation_date  292706 non-null  datetime64[ns]\n 16  Diagnose         292706 non-null  int64         \n 17  Adnum            292706 non-null  int64         \n 18  idx_1hr          292706 non-null  object        \ndtypes: datetime64[ns](2), float16(12), int64(4), object(1)\nmemory usage: 23.6+ MB\nNone\n"
     ]
    }
   ],
   "source": [
    "pat=pd.read_excel(r'Results_CHD\\admissiondate_CHD0.xlsx',\n",
    "                    parse_dates=['OK_datum'],index_col=0)\n",
    "group=pat.groupby('pat_hosp_id',sort=False).max().reset_index()\n",
    "group.drop_duplicates('pat_hosp_id',inplace=True)\n",
    "\n",
    "df_train,df_val,df_test=pp.split_stratified_into_train_val_test(group, stratify_colname='Reintub',\n",
    "                                         frac_train=0.7, frac_val=0.1, frac_test=0.2,\n",
    "                                         random_state=1)\n",
    "\n",
    "train_pat=df_train['pat_hosp_id'].unique()\n",
    "test_pat=df_test['pat_hosp_id'].unique()\n",
    "val_pat=df_val['pat_hosp_id'].unique()\n",
    "\n",
    "df_train = df[df.index.get_level_values('pat_hosp_id').isin(train_pat)]\n",
    "df_val = df[df.index.get_level_values('pat_hosp_id').isin(val_pat)]\n",
    "df_test = df[df.index.get_level_values('pat_hosp_id').isin(test_pat)]\n",
    "print(df_train.info())\n",
    "print(df_test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_modelling(df):\n",
    "    y = df[['Reintub', 'Adnum', 'idx_1hr']]\n",
    "    y = y.groupby(['Adnum', 'idx_1hr'], sort=False).agg(['max'])\n",
    "    y.columns = [\"_\".join(a) for a in y.columns.to_flat_index()]\n",
    "    y.reset_index(drop=True, inplace=True, level='idx_1hr')\n",
    "    y = y.reset_index().drop_duplicates().set_index(\n",
    "        ['Adnum'])\n",
    "    y = y[~y.index.duplicated(keep='last')]\n",
    "    return y\n",
    "Y_TRAIN=y_modelling(df_train)\n",
    "Y_TEST=y_modelling(df_test)\n",
    "Y_VAL=y_modelling(df_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reintub_max\n0.0            1477\n1.0              90\ndtype: int64\nReintub_max\n0.0            209\n1.0             13\ndtype: int64\nReintub_max\n0.0            410\n1.0             27\ndtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(Y_TRAIN.value_counts())\n",
    "print(Y_VAL.value_counts())\n",
    "print(Y_TEST.value_counts())\n",
    "Y_TRAIN=Y_TRAIN['Reintub_max'].to_numpy()\n",
    "Y_TEST=Y_TEST['Reintub_max'].to_numpy()\n",
    "Y_VAL=Y_VAL['Reintub_max'].to_numpy()\n",
    "Y_TRAIN=np.append(Y_TRAIN,Y_VAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1567 entries, 1484 to 1482\nColumns: 243 entries, mon_etco2_mean_1 to Diagnose_mean\ndtypes: float64(242), int64(1)\nmemory usage: 2.9 MB\nNone\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 437 entries, 1483 to 1473\nColumns: 243 entries, mon_etco2_mean_1 to Diagnose_mean\ndtypes: float64(241), int64(2)\nmemory usage: 849.2 KB\nNone\n"
     ]
    }
   ],
   "source": [
    "def x_modelling(df):\n",
    "    temp = df[['Age', 'Adnum', 'idx_1hr','pat_weight_act','Diagnose']]\n",
    "    df = df.drop(['Age','pat_weight_act','Extubation_date','level_0','pat_datetime', 'Reintub','Diagnose'], axis=1)\n",
    "    df = df.groupby(['Adnum', 'idx_1hr'], sort=False).agg(['mean', 'std'])\n",
    "    df.columns = [\"_\".join(a) for a in df.columns.to_flat_index()]\n",
    "    df = df.stack().unstack([2, 1])\n",
    "    df.columns = [\"_\".join(a) for a in df.columns.to_flat_index()]\n",
    "    df = df.reset_index().drop_duplicates().set_index('Adnum')\n",
    "    df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "    temp = temp.groupby('Adnum', sort=False).agg(['mean'])\n",
    "    temp.columns = [\"_\".join(a) for a in temp.columns.to_flat_index()]\n",
    "    temp = temp.reset_index().drop_duplicates().set_index('Adnum')\n",
    "    temp = temp[~temp.index.duplicated(keep='last')]\n",
    "    df = df.merge(temp, right_index=True, left_index=True, how='left')\n",
    "    return df\n",
    "X_TRAIN=memory_upscale(x_modelling(df_train))\n",
    "X_TEST=memory_upscale(x_modelling(df_test))\n",
    "X_VAL=memory_upscale(x_modelling(df_val))\n",
    "print(X_TRAIN.info())\n",
    "print(X_TEST.info())\n",
    "X_TRAIN=X_TRAIN.fillna(method='ffill').fillna(method='bfill')\n",
    "X_VAL=X_VAL.fillna(method='ffill').fillna(method='bfill')\n",
    "X_TEST=X_TEST.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "X_TRAIN=X_TRAIN.append(X_VAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.path.join(output_folder, 'x_train.txt'), 'wb')\n",
    "pickle.dump(X_TRAIN, f)\n",
    "f.close()\n",
    "\n",
    "f = open(os.path.join(output_folder, 'x_test.txt'), 'wb')\n",
    "pickle.dump(X_TEST, f)\n",
    "f.close()\n",
    "\n",
    "f = open(os.path.join(output_folder, 'y_train.txt'), 'wb')\n",
    "pickle.dump(Y_TRAIN, f)\n",
    "f.close()\n",
    "\n",
    "f = open(os.path.join(output_folder, 'y_test.txt'), 'wb')\n",
    "pickle.dump(Y_TEST, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(), n_iter=20,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 14, 18, 22, 26, 30,\n",
       "                                                      34, 38, 42, 46, 50,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 50, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 20, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_TRAIN, Y_TRAIN)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1789 entries, 1484 to 1481\nColumns: 243 entries, mon_etco2_mean_1 to Diagnose_mean\ndtypes: float64(242), int64(1)\nmemory usage: 3.3 MB\nNone\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 437 entries, 1483 to 1473\nColumns: 243 entries, mon_etco2_mean_1 to Diagnose_mean\ndtypes: float64(241), int64(2)\nmemory usage: 849.2 KB\nNone\n"
     ]
    }
   ],
   "source": [
    "print(X_TRAIN.info())\n",
    "print(X_TEST.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model Performance\n",
      "average_precision = 0.2806697959721222\n",
      "F1 score: 0.13333333333333333 \n",
      "AUC = 0.7817524841915087\n",
      "Model Performance\n",
      "average_precision = 0.4016124888026592\n",
      "F1 score: 0.07142857142857142 \n",
      "AUC = 0.8523486901535682\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix,average_precision_score,f1_score,roc_curve,roc_auc_score,plot_confusion_matrix\n",
    "\n",
    "base_model = RandomForestClassifier(n_estimators = 10, random_state = 42,max_depth=10)\n",
    "base_model.fit(X_TRAIN, Y_TRAIN)\n",
    "base_accuracy = evaluate(base_model, X_TEST, Y_TEST,'base_accuracy RF',output_folder)\n",
    "\n",
    "best_random = rf_random.best_estimator_\n",
    "random_accuracy = evaluate(best_random, X_TEST,Y_TEST,'Best_random RF',output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Brier score losses: (the smaller the better)\nNo calibration: 0.048\nWith isotonic calibration: 0.049\nWith sigmoid calibration: 0.048\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from matplotlib import cm\n",
    "\n",
    "# Gaussian Naive-Bayes with no calibration\n",
    "# GaussianNB itself does not support sample-weights\n",
    "prob_pos_clf = best_random.predict_proba(X_TEST)[:, 1]\n",
    "\n",
    "# Gaussian Naive-Bayes with isotonic calibration\n",
    "clf_isotonic = CalibratedClassifierCV(best_random, cv=2, method='isotonic')\n",
    "clf_isotonic.fit(X_TRAIN, Y_TRAIN)\n",
    "prob_pos_isotonic = clf_isotonic.predict_proba(X_TEST)[:, 1]\n",
    "\n",
    "# Gaussian Naive-Bayes with sigmoid calibration\n",
    "clf_sigmoid = CalibratedClassifierCV(best_random, cv=2, method='sigmoid')\n",
    "clf_sigmoid.fit(X_TRAIN, Y_TRAIN)\n",
    "prob_pos_sigmoid = clf_sigmoid.predict_proba(X_TEST)[:, 1]\n",
    "\n",
    "print(\"Brier score losses: (the smaller the better)\")\n",
    "\n",
    "clf_score = brier_score_loss(Y_TEST, prob_pos_clf)\n",
    "print(\"No calibration: %1.3f\" % clf_score)\n",
    "\n",
    "clf_isotonic_score = brier_score_loss(Y_TEST, prob_pos_isotonic)\n",
    "print(\"With isotonic calibration: %1.3f\" % clf_isotonic_score)\n",
    "\n",
    "clf_sigmoid_score = brier_score_loss(Y_TEST, prob_pos_sigmoid)\n",
    "print(\"With sigmoid calibration: %1.3f\" % clf_sigmoid_score)\n",
    "\n",
    "with open(os.path.join(output_folder,f\"Result_scores_all.txt\"),'a') as file:\n",
    "    file.write(f\"Results for RF on training set\\n\\n\")\n",
    "    file.write(\"Brier score losses: (the smaller the better)\\n\")\n",
    "    file.write(\"No calibration: %1.3f\" % clf_score)\n",
    "    file.write(\"With isotonic calibration: %1.3f\" % clf_isotonic_score)\n",
    "    file.write(\"With sigmoid calibration: %1.3f\" % clf_sigmoid_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Confidence interval for the score: [0.780 - 0.912]\n(0.7606475223472423, 0.944049857959894)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix,average_precision_score,f1_score,roc_curve,roc_auc_score,plot_confusion_matrix\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "from all_own_funct import roc_auc_ci\n",
    "try:\n",
    "    pdf = PdfPages(os.path.join(output_folder,f\"Figures_alls.pdf\"))\n",
    "except PermissionError:\n",
    "    os.remove(os.path.join(output_folder,f\"Figures_all.pdf\"))\n",
    "    os.remove(os.path.join(output_folder,f\"Result_scores_all.txt\"))\n",
    "clf = best_random \n",
    "\n",
    "\n",
    "y_pred_clas=clf.predict(X_TEST)\n",
    "# Predict the probabilities, function depends on used classifier\n",
    "\n",
    "try:\n",
    "    y_pred_prob=clf.predict_proba(X_TEST)\n",
    "    y_pred_prob=y_pred_prob[:,1]\n",
    "except:\n",
    "    try:\n",
    "        y_pred_prob=clf.decision_function(X_TEST)\n",
    "    except:\n",
    "        y_pred_prob=y_pred_clas\n",
    "\n",
    "report=classification_report(Y_TEST,y_pred_clas,target_names=['No Reintubation','Reintubation'])\n",
    "score=clf.score(X_TEST,Y_TEST)\n",
    "average_precision = average_precision_score(Y_TEST, y_pred_prob)\n",
    "f1_s=f1_score(Y_TEST, y_pred_clas)\n",
    "\n",
    "# write scoring metrics to file\n",
    "with open(os.path.join(output_folder,f\"Result_scores_all.txt\"),'a') as file:\n",
    "    file.write(f\"Results for RF on training set\\n\\n\")\n",
    "    file.write(f\"Classification report \\n {report} \\n\")\n",
    "    file.write(f\"Hold_out_scores {score} \\n\")\n",
    "    file.write(f\"Average precision score {average_precision} \\n\")\n",
    "    file.write(f\"F1 score {f1_s} \\n\\n\\n\")\n",
    "\n",
    "plot_confusion_matrix(clf,X_TEST,Y_TEST)\n",
    "plt.title(f\"Confusion matrix of random forrest\")\n",
    "fig=plt.gcf()\n",
    "pdf.savefig(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(Y_TEST,  y_pred_prob)\n",
    "auc = roc_auc_score(Y_TEST, y_pred_prob)\n",
    "\n",
    "n_bootstraps = 2000\n",
    "rng_seed = 42  # control reproducibility\n",
    "bootstrapped_scores = []\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = rng.randint(0, len(y_pred_prob), len(y_pred_prob))\n",
    "    if len(np.unique(Y_TEST[indices])) < 2:\n",
    "        # We need at least one positive and one negative sample for ROC AUC\n",
    "        # to be defined: reject the sample\n",
    "        continue\n",
    "\n",
    "    score = roc_auc_score(Y_TEST[indices], y_pred_prob[indices])\n",
    "    bootstrapped_scores.append(score)\n",
    "    #print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))\n",
    "\n",
    "sorted_scores = np.array(bootstrapped_scores)\n",
    "sorted_scores.sort()\n",
    "\n",
    "# Computing the lower and upper bound of the 90% confidence interval\n",
    "# You can change the bounds percentiles to 0.025 and 0.975 to get\n",
    "# a 95% confidence interval instead.\n",
    "confidence_lower = sorted_scores[int(0.05 * len(sorted_scores))]\n",
    "confidence_upper = sorted_scores[int(0.95 * len(sorted_scores))]\n",
    "with open(os.path.join(output_folder,f\"Result_scores_all.txt\"),'a') as file:\n",
    "    file.write(\"\\nConfidence interval for the score: [{:0.3f} - {:0.3}]\\n\".format(\n",
    "    confidence_lower, confidence_upper))\n",
    "a=roc_auc_ci(Y_TEST,y_pred_prob)\n",
    "print(a)\n",
    "\n",
    "\n",
    "plt.plot(fpr,tpr,label=f\"auc={auc}\",linewidth=1.5,markersize=1)\n",
    "\n",
    "plt.legend(loc=4,fontsize='xx-small')\n",
    "plt.title(f'ROC of Random Forrest hour data')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,1])\n",
    "axes.set_ylim([0,1])\n",
    "fig=plt.gcf()\n",
    "pdf.savefig(fig)\n",
    "plt.close(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:   36.0s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  60 | elapsed:   36.3s remaining:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  1.0min finished\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=LogisticRegression(max_iter=1000), n_iter=20,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'C': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n",
       "                                        'class_weight': [{0: 0.1, 1: 0.9},\n",
       "                                                         'balanced'],\n",
       "                                        'penalty': ['l1', 'l2', 'elasticnet'],\n",
       "                                        'solver': ['newton-cg', 'lbfgs',\n",
       "                                                   'liblinear', 'sag',\n",
       "                                                   'saga']},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "# Number of features to consider at every split\n",
    "solver =['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "# Maximum number of levels in tree\n",
    "class_weight=[{0:0.1,1:0.9},'balanced']\n",
    "# Minimum number of samples required to split a node\n",
    "penalty=['l1', 'l2', 'elasticnet']\n",
    "# Minimum number of samples required at each leaf node\n",
    "C=np.logspace(-3,3,7)\n",
    "# Method of selecting samples for training each tree\n",
    "# Create the random grid\n",
    "random_grid = {'solver':solver,\n",
    "                'class_weight':class_weight,\n",
    "                'penalty':penalty,\n",
    "                'C':C}\n",
    "\n",
    "# First create the base model to tune\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "lr_random = RandomizedSearchCV(estimator = lr , param_distributions = random_grid, n_iter = 20, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "lr_random.fit(X_TRAIN, Y_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\berend\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "Model Performance\n",
      "average_precision = 0.27482673146387465\n",
      "F1 score: 0.15384615384615383 \n",
      "AUC = 0.7913279132791327\n",
      "Model Performance\n",
      "average_precision = 0.15925805213079813\n",
      "F1 score: 0.2564102564102564 \n",
      "AUC = 0.6233965672990063\n",
      "Brier score losses: (the smaller the better)\n",
      "No calibration: 0.118\n",
      "With isotonic calibration: 0.056\n",
      "With sigmoid calibration: 0.058\n"
     ]
    }
   ],
   "source": [
    "base_model_lr = LogisticRegression(max_iter=1000)\n",
    "base_model_lr.fit(X_TRAIN, Y_TRAIN)\n",
    "base_accuracy_lr = evaluate(base_model_lr, X_TEST, Y_TEST,'base LR',output_folder)\n",
    "\n",
    "best_random_lr = lr_random.best_estimator_\n",
    "random_accuracy_lr = evaluate(best_random_lr, X_TEST,Y_TEST,'Best Random LR',output_folder)\n",
    "\n",
    "\n",
    "# Gaussian Naive-Bayes with no calibration\n",
    "# GaussianNB itself does not support sample-weights\n",
    "prob_pos_clf = best_random_lr.predict_proba(X_TEST)[:, 1]\n",
    "\n",
    "# Gaussian Naive-Bayes with isotonic calibration\n",
    "clf_isotonic = CalibratedClassifierCV(best_random_lr, cv=2, method='isotonic')\n",
    "clf_isotonic.fit(X_TRAIN, Y_TRAIN)\n",
    "prob_pos_isotonic = clf_isotonic.predict_proba(X_TEST)[:, 1]\n",
    "\n",
    "# Gaussian Naive-Bayes with sigmoid calibration\n",
    "clf_sigmoid = CalibratedClassifierCV(best_random_lr, cv=2, method='sigmoid')\n",
    "clf_sigmoid.fit(X_TRAIN, Y_TRAIN)\n",
    "prob_pos_sigmoid = clf_sigmoid.predict_proba(X_TEST)[:, 1]\n",
    "\n",
    "print(\"Brier score losses: (the smaller the better)\")\n",
    "\n",
    "clf_score = brier_score_loss(Y_TEST, prob_pos_clf)\n",
    "print(\"No calibration: %1.3f\" % clf_score)\n",
    "\n",
    "clf_isotonic_score = brier_score_loss(Y_TEST, prob_pos_isotonic)\n",
    "print(\"With isotonic calibration: %1.3f\" % clf_isotonic_score)\n",
    "\n",
    "clf_sigmoid_score = brier_score_loss(Y_TEST, prob_pos_sigmoid)\n",
    "print(\"With sigmoid calibration: %1.3f\" % clf_sigmoid_score)\n",
    "\n",
    "with open(os.path.join(output_folder,f\"Result_scores_all.txt\"),'a') as file:\n",
    "    file.write(f\"Results for LR on training set\\n\\n\")\n",
    "    file.write(\"Brier score losses: (the smaller the better)\\n\")\n",
    "    file.write(\"No calibration: %1.3f\" % clf_score)\n",
    "    file.write(\"With isotonic calibration: %1.3f\" % clf_isotonic_score)\n",
    "    file.write(\"With sigmoid calibration: %1.3f\" % clf_sigmoid_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Confidence interval for the score: [0.507 - 0.731]\n(0.5073920628052035, 0.739401071792809)\n"
     ]
    }
   ],
   "source": [
    "clf = best_random_lr\n",
    "\n",
    "\n",
    "y_pred_clas=clf.predict(X_TEST)\n",
    "# Predict the probabilities, function depends on used classifier\n",
    "\n",
    "try:\n",
    "    y_pred_prob=clf.predict_proba(X_TEST)\n",
    "    y_pred_prob=y_pred_prob[:,1]\n",
    "except:\n",
    "    try:\n",
    "        y_pred_prob=clf.decision_function(X_TEST)\n",
    "    except:\n",
    "        y_pred_prob=y_pred_clas\n",
    "\n",
    "report=classification_report(Y_TEST,y_pred_clas,target_names=['No Reintubation','Reintubation'])\n",
    "score=clf.score(X_TEST,Y_TEST)\n",
    "average_precision = average_precision_score(Y_TEST, y_pred_prob)\n",
    "f1_s=f1_score(Y_TEST, y_pred_clas)\n",
    "\n",
    "with open(os.path.join(output_folder,f\"Result_scores_all.txt\"),'a') as file:\n",
    "    file.write(f\"Results for LR on training\\n\\n\")\n",
    "    file.write(f\"Classification report \\n {report} \\n\")\n",
    "    file.write(f\"Hold_out_scores {score} \\n\")\n",
    "    file.write(f\"Average precision score {average_precision} \\n\")\n",
    "    file.write(f\"F1 score {f1_s} \\n\\n\\n\")\n",
    "\n",
    "plot_confusion_matrix(clf,X_TEST,Y_TEST)\n",
    "plt.title(f\"Confusion matrix of logistic regression\")\n",
    "fig=plt.gcf()\n",
    "pdf.savefig(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(Y_TEST,  y_pred_prob)\n",
    "auc = roc_auc_score(Y_TEST, y_pred_prob)\n",
    "\n",
    "\n",
    "n_bootstraps = 2000\n",
    "rng_seed = 42  # control reproducibility\n",
    "bootstrapped_scores = []\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = rng.randint(0, len(y_pred_prob), len(y_pred_prob))\n",
    "    if len(np.unique(Y_TEST[indices])) < 2:\n",
    "        # We need at least one positive and one negative sample for ROC AUC\n",
    "        # to be defined: reject the sample\n",
    "        continue\n",
    "\n",
    "    score = roc_auc_score(Y_TEST[indices], y_pred_prob[indices])\n",
    "    bootstrapped_scores.append(score)\n",
    "    #print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))\n",
    "\n",
    "sorted_scores = np.array(bootstrapped_scores)\n",
    "sorted_scores.sort()\n",
    "\n",
    "# Computing the lower and upper bound of the 90% confidence interval\n",
    "# You can change the bounds percentiles to 0.025 and 0.975 to get\n",
    "# a 95% confidence interval instead.\n",
    "confidence_lower = sorted_scores[int(0.05 * len(sorted_scores))]\n",
    "confidence_upper = sorted_scores[int(0.95 * len(sorted_scores))]\n",
    "with open(os.path.join(output_folder,f\"Result_scores_all.txt\"),'a') as file:\n",
    "    file.write(\"\\nConfidence interval for the score: [{:0.3f} - {:0.3}]\\n\".format(\n",
    "    confidence_lower, confidence_upper))\n",
    "\n",
    "a=roc_auc_ci(Y_TEST,y_pred_prob)\n",
    "print(a)\n",
    "\n",
    "plt.plot(fpr,tpr,label=f\"auc={auc}\",linewidth=1.5,markersize=1)\n",
    "\n",
    "\n",
    "plt.legend(loc=4,fontsize='xx-small')\n",
    "plt.title(f'ROC of Logistic Regression data')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,1])\n",
    "axes.set_ylim([0,1])\n",
    "fig=plt.gcf()\n",
    "pdf.savefig(fig)\n",
    "plt.close(fig)\n",
    "pdf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open(os.path.join(output_folder,'ran_for.sav'), 'wb')\n",
    "pickle.dump(best_random, f)\n",
    "f.close()\n",
    "\n",
    "f = open(os.path.join(output_folder,'log_reg.sav'), 'wb')\n",
    "pickle.dump(best_random_lr, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}