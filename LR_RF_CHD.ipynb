{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "3b7e431a861a72513f361682db481bb043ab213b57189a56d42fe6b32fa57c58"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\berend\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\nC:\\Users\\berend\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\nC:\\Users\\berend\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\nC:\\Users\\berend\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from all_stand_var import conv_dict, lab_cols, used_cols\n",
    "from all_own_funct import memory_downscale,memory_upscale,evaluate,lin_reg_coef\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "from seaborn import heatmap\n",
    "from sklearn.metrics import roc_curve, accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix,average_precision_score,f1_score,roc_curve,roc_auc_score,plot_confusion_matrix\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from scipy.stats import kurtosis\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import keras.backend as K\n",
    "from RNN_LTSM_CHD import return_loaded_model\n",
    "import pickle\n",
    "import locale\n",
    "import LR_build_CHD as pp\n",
    "import tables\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve, accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "locale.setlocale(locale.LC_ALL, 'fr_FR')\n",
    "\n",
    "# output folder in which al results are saved\n",
    "output_folder = os.path.join(os.getcwd(), 'Results_LR_RF_CHD_v3','dynamic')\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nMultiIndex: 978473 entries, (0, 7171037, Timestamp('2008-04-11 00:00:00')) to (77, 3536787, Timestamp('2020-01-10 00:00:00'))\nData columns (total 17 columns):\n #   Column           Non-Null Count   Dtype         \n---  ------           --------------   -----         \n 0   pat_datetime     978473 non-null  datetime64[ns]\n 1   pat_weight_act   978473 non-null  float16       \n 2   mon_etco2        829237 non-null  float16       \n 3   mon_hr           971134 non-null  float16       \n 4   mon_ibp_mean     887213 non-null  float16       \n 5   mon_rr           931288 non-null  float16       \n 6   mon_sat          959128 non-null  float16       \n 7   vent_m_fio2      973620 non-null  float16       \n 8   vent_m_peep      960680 non-null  float16       \n 9   vent_m_ppeak     977208 non-null  float16       \n 10  vent_m_rr        978466 non-null  float16       \n 11  vent_m_tv_exp    972433 non-null  float16       \n 12  Age              978473 non-null  float16       \n 13  Reintub          978473 non-null  float16       \n 14  Extubation_date  978473 non-null  datetime64[ns]\n 15  Diagnose         978473 non-null  int64         \n 16  Adnum            978473 non-null  int64         \ndtypes: datetime64[ns](2), float16(13), int64(2)\nmemory usage: 60.0 MB\nNone\n"
     ]
    }
   ],
   "source": [
    "import tables\n",
    "# Read in data\n",
    "df=pd.read_hdf(os.path.join('./Results_CHD_v5/final', 'processed_df.h5'),key='df',mode='r')\n",
    "#df=memory_downscale(df)\n",
    "print(df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 978473 entries, 0 to 978472\n",
      "Data columns (total 20 columns):\n",
      " #   Column           Non-Null Count   Dtype         \n",
      "---  ------           --------------   -----         \n",
      " 0   level_0          978473 non-null  int64         \n",
      " 1   pat_hosp_id      978473 non-null  int64         \n",
      " 2   OK_datum         978473 non-null  datetime64[ns]\n",
      " 3   pat_datetime     978473 non-null  datetime64[ns]\n",
      " 4   pat_weight_act   978473 non-null  float16       \n",
      " 5   mon_etco2        829237 non-null  float16       \n",
      " 6   mon_hr           971134 non-null  float16       \n",
      " 7   mon_ibp_mean     887213 non-null  float16       \n",
      " 8   mon_rr           931288 non-null  float16       \n",
      " 9   mon_sat          959128 non-null  float16       \n",
      " 10  vent_m_fio2      973620 non-null  float16       \n",
      " 11  vent_m_peep      960680 non-null  float16       \n",
      " 12  vent_m_ppeak     977208 non-null  float16       \n",
      " 13  vent_m_rr        978466 non-null  float16       \n",
      " 14  vent_m_tv_exp    972433 non-null  float16       \n",
      " 15  Age              978473 non-null  float16       \n",
      " 16  Reintub          978473 non-null  float16       \n",
      " 17  Extubation_date  978473 non-null  datetime64[ns]\n",
      " 18  Diagnose         978473 non-null  int64         \n",
      " 19  Adnum            978473 non-null  int64         \n",
      "dtypes: datetime64[ns](3), float16(13), int64(4)\n",
      "memory usage: 76.5 MB\n",
      "['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12']\n"
     ]
    }
   ],
   "source": [
    "# calculate the hour intervals, and also the missingness parameter\n",
    "\n",
    "df=df.reset_index(drop=False)\n",
    "\n",
    "def index_1hr(group):\n",
    "    if len(group) > 720:\n",
    "        group=group.sort_values('pat_datetime')\n",
    "        group=group.tail(720)\n",
    "   \n",
    "    group['tim']=pd.date_range(start='1/1/2018', periods=len(group), freq='1min')\n",
    "    grouped = pd.Grouper(key='tim', freq='1H')\n",
    "    group['idx_1hr']=group.groupby(grouped,sort=False).ngroup().add(1)\n",
    "    group['mis']=(group['pat_datetime'].max()-group['pat_datetime'].min()).total_seconds()/60\n",
    "    del group['tim']\n",
    "    return group\n",
    "\n",
    "df=df.groupby('Adnum',sort=False,as_index=False).apply(index_1hr)\n",
    "df['idx_1hr']=df['idx_1hr'].astype(str)\n",
    "print(df['idx_1hr'].unique())\n",
    "df[['mis']]=StandardScaler().fit_transform(df[['mis']])\n",
    "df=df.set_index(['pat_hosp_id','OK_datum'])\n",
    "df=df.sort_values('idx_1hr',ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nMultiIndex: 688151 entries, (818812, Timestamp('2012-01-23 00:00:00')) to (7171037, Timestamp('2008-04-11 00:00:00'))\nData columns (total 20 columns):\n #   Column           Non-Null Count   Dtype         \n---  ------           --------------   -----         \n 0   level_0          688151 non-null  int64         \n 1   pat_datetime     688151 non-null  datetime64[ns]\n 2   pat_weight_act   688151 non-null  float16       \n 3   mon_etco2        582366 non-null  float16       \n 4   mon_hr           682682 non-null  float16       \n 5   mon_ibp_mean     626468 non-null  float16       \n 6   mon_rr           653734 non-null  float16       \n 7   mon_sat          674427 non-null  float16       \n 8   vent_m_fio2      683642 non-null  float16       \n 9   vent_m_peep      674252 non-null  float16       \n 10  vent_m_ppeak     687071 non-null  float16       \n 11  vent_m_rr        688145 non-null  float16       \n 12  vent_m_tv_exp    685418 non-null  float16       \n 13  Age              688151 non-null  float16       \n 14  Reintub          688151 non-null  float16       \n 15  Extubation_date  688151 non-null  datetime64[ns]\n 16  Diagnose         688151 non-null  int64         \n 17  Adnum            688151 non-null  int64         \n 18  idx_1hr          688151 non-null  object        \n 19  mis              688151 non-null  float64       \ndtypes: datetime64[ns](2), float16(13), float64(1), int64(3), object(1)\nmemory usage: 56.6+ MB\nNone\n<class 'pandas.core.frame.DataFrame'>\nMultiIndex: 194775 entries, (95291, Timestamp('2015-04-17 00:00:00')) to (5948194, Timestamp('2013-10-11 00:00:00'))\nData columns (total 20 columns):\n #   Column           Non-Null Count   Dtype         \n---  ------           --------------   -----         \n 0   level_0          194775 non-null  int64         \n 1   pat_datetime     194775 non-null  datetime64[ns]\n 2   pat_weight_act   194775 non-null  float16       \n 3   mon_etco2        164527 non-null  float16       \n 4   mon_hr           193553 non-null  float16       \n 5   mon_ibp_mean     174989 non-null  float16       \n 6   mon_rr           186350 non-null  float16       \n 7   mon_sat          191368 non-null  float16       \n 8   vent_m_fio2      194652 non-null  float16       \n 9   vent_m_peep      193282 non-null  float16       \n 10  vent_m_ppeak     194646 non-null  float16       \n 11  vent_m_rr        194775 non-null  float16       \n 12  vent_m_tv_exp    192481 non-null  float16       \n 13  Age              194775 non-null  float16       \n 14  Reintub          194775 non-null  float16       \n 15  Extubation_date  194775 non-null  datetime64[ns]\n 16  Diagnose         194775 non-null  int64         \n 17  Adnum            194775 non-null  int64         \n 18  idx_1hr          194775 non-null  object        \n 19  mis              194775 non-null  float64       \ndtypes: datetime64[ns](2), float16(13), float64(1), int64(3), object(1)\nmemory usage: 16.1+ MB\nNone\n"
     ]
    }
   ],
   "source": [
    "# split dataframe into train, test and validation set, stratified\n",
    "pat=pd.read_excel(r'Results_CHD\\admissiondate_CHD0.xlsx',\n",
    "                    parse_dates=['OK_datum'],index_col=0)\n",
    "group=pat.groupby('pat_hosp_id',sort=False).max().reset_index()\n",
    "group.drop_duplicates('pat_hosp_id',inplace=True)\n",
    "\n",
    "df_train,df_val,df_test=pp.split_stratified_into_train_val_test(group, stratify_colname='Reintub',\n",
    "                                         frac_train=0.7, frac_val=0.1, frac_test=0.2,\n",
    "                                         random_state=1)\n",
    "\n",
    "train_pat=df_train['pat_hosp_id'].unique()\n",
    "test_pat=df_test['pat_hosp_id'].unique()\n",
    "val_pat=df_val['pat_hosp_id'].unique()\n",
    "\n",
    "df_train = df[df.index.get_level_values('pat_hosp_id').isin(train_pat)]\n",
    "df_val = df[df.index.get_level_values('pat_hosp_id').isin(val_pat)]\n",
    "df_test = df[df.index.get_level_values('pat_hosp_id').isin(test_pat)]\n",
    "print(df_train.info())\n",
    "print(df_test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_modelling(df):\n",
    "    y = df[['Reintub', 'Adnum', 'idx_1hr']]\n",
    "    y = y.groupby(['Adnum', 'idx_1hr'], sort=False).agg(['max'])\n",
    "    y.columns = [\"_\".join(a) for a in y.columns.to_flat_index()]\n",
    "    y.reset_index(drop=True, inplace=True, level='idx_1hr')\n",
    "    y = y.reset_index().drop_duplicates().set_index(\n",
    "        ['Adnum'])\n",
    "    y = y[~y.index.duplicated(keep='last')]\n",
    "    return y\n",
    "# from dataframwe with label per minute, to array with label per admission\n",
    "\n",
    "Y_TRAIN=y_modelling(df_train)\n",
    "Y_TEST=y_modelling(df_test)\n",
    "Y_VAL=y_modelling(df_val)\n",
    "print(Y_TRAIN.value_counts())\n",
    "print(Y_VAL.value_counts())\n",
    "print(Y_TEST.value_counts())\n",
    "Y_TRAIN=Y_TRAIN['Reintub_max'].to_numpy()\n",
    "Y_TEST=Y_TEST['Reintub_max'].to_numpy()\n",
    "Y_VAL=Y_VAL['Reintub_max'].to_numpy()\n",
    "Y_TRAIN=np.append(Y_TRAIN,Y_VAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "c:\\Users\\berend\\Documents\\Python_Scripts\\Patient_Selection\\all_own_funct.py:119: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].astype('float64')\n",
      "c:\\Users\\berend\\Documents\\Python_Scripts\\Patient_Selection\\all_own_funct.py:123: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].astype('int64')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 688151 entries, (818812, Timestamp('2012-01-23 00:00:00')) to (7171037, Timestamp('2008-04-11 00:00:00'))\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   mon_etco2      582366 non-null  float64\n",
      " 1   mon_hr         682682 non-null  float64\n",
      " 2   mon_ibp_mean   626468 non-null  float64\n",
      " 3   mon_rr         653734 non-null  float64\n",
      " 4   mon_sat        674427 non-null  float64\n",
      " 5   vent_m_fio2    683642 non-null  float64\n",
      " 6   vent_m_peep    674252 non-null  float64\n",
      " 7   vent_m_ppeak   687071 non-null  float64\n",
      " 8   vent_m_rr      688145 non-null  float64\n",
      " 9   vent_m_tv_exp  685418 non-null  float64\n",
      " 10  Adnum          688151 non-null  int64  \n",
      " 11  idx_1hr        688151 non-null  object \n",
      "dtypes: float64(10), int64(1), object(1)\n",
      "memory usage: 65.8+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1514 entries, 700 to 1055\n",
      "Data columns (total 4 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Age_mean             1514 non-null   float64\n",
      " 1   pat_weight_act_mean  1514 non-null   float64\n",
      " 2   Diagnose_mean        1514 non-null   float64\n",
      " 3   mis_mean             1514 non-null   float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 59.1 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 194775 entries, (95291, Timestamp('2015-04-17 00:00:00')) to (5948194, Timestamp('2013-10-11 00:00:00'))\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   mon_etco2      164527 non-null  float64\n",
      " 1   mon_hr         193553 non-null  float64\n",
      " 2   mon_ibp_mean   174989 non-null  float64\n",
      " 3   mon_rr         186350 non-null  float64\n",
      " 4   mon_sat        191368 non-null  float64\n",
      " 5   vent_m_fio2    194652 non-null  float64\n",
      " 6   vent_m_peep    193282 non-null  float64\n",
      " 7   vent_m_ppeak   194646 non-null  float64\n",
      " 8   vent_m_rr      194775 non-null  float64\n",
      " 9   vent_m_tv_exp  192481 non-null  float64\n",
      " 10  Adnum          194775 non-null  int64  \n",
      " 11  idx_1hr        194775 non-null  object \n",
      "dtypes: float64(10), int64(1), object(1)\n",
      "memory usage: 18.7+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 423 entries, 1422 to 1142\n",
      "Data columns (total 4 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Age_mean             423 non-null    float64\n",
      " 1   pat_weight_act_mean  423 non-null    float64\n",
      " 2   Diagnose_mean        423 non-null    int64  \n",
      " 3   mis_mean             423 non-null    float64\n",
      "dtypes: float64(3), int64(1)\n",
      "memory usage: 16.5 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 94827 entries, (9012287, Timestamp('2014-04-14 00:00:00')) to (9332033, Timestamp('2018-03-28 00:00:00'))\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   mon_etco2      81625 non-null  float64\n",
      " 1   mon_hr         94179 non-null  float64\n",
      " 2   mon_ibp_mean   85036 non-null  float64\n",
      " 3   mon_rr         90484 non-null  float64\n",
      " 4   mon_sat        92621 non-null  float64\n",
      " 5   vent_m_fio2    94606 non-null  float64\n",
      " 6   vent_m_peep    92426 non-null  float64\n",
      " 7   vent_m_ppeak   94771 non-null  float64\n",
      " 8   vent_m_rr      94826 non-null  float64\n",
      " 9   vent_m_tv_exp  93814 non-null  float64\n",
      " 10  Adnum          94827 non-null  int64  \n",
      " 11  idx_1hr        94827 non-null  object \n",
      "dtypes: float64(10), int64(1), object(1)\n",
      "memory usage: 9.2+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 212 entries, 1192 to 1018\n",
      "Data columns (total 4 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Age_mean             212 non-null    float64\n",
      " 1   pat_weight_act_mean  212 non-null    float64\n",
      " 2   Diagnose_mean        212 non-null    int64  \n",
      " 3   mis_mean             212 non-null    float64\n",
      "dtypes: float64(3), int64(1)\n",
      "memory usage: 8.3 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1514 entries, 700 to 1055\n",
      "Columns: 250 entries, mon_hr_mean_9 to vent_m_tv_exp_lin_reg_coef\n",
      "dtypes: float64(250)\n",
      "memory usage: 2.9 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 423 entries, 1422 to 1142\n",
      "Columns: 250 entries, mon_etco2_mean_9 to vent_m_tv_exp_lin_reg_coef\n",
      "dtypes: float64(250)\n",
      "memory usage: 845.6 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def x_modelling(df):\n",
    "    temp = df[['Age', 'Adnum', 'idx_1hr','pat_weight_act','Diagnose','mis']]\n",
    "    df = df.drop(['Age','pat_weight_act','Extubation_date','level_0','pat_datetime', 'Reintub','Diagnose','mis'], axis=1)\n",
    "    print(df.info())\n",
    "    df_temp=df\n",
    "    df = df.groupby(['Adnum', 'idx_1hr'], sort=False).agg(['mean', 'std'])\n",
    "    df.columns = [\"_\".join(a) for a in df.columns.to_flat_index()]\n",
    "    df = df.stack().unstack([2, 1])\n",
    "    df.columns = [\"_\".join(a) for a in df.columns.to_flat_index()]\n",
    "    df = df.reset_index().drop_duplicates().set_index('Adnum')\n",
    "    df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "    temp = temp.groupby('Adnum', sort=False).agg(['mean'])\n",
    "    temp.columns = [\"_\".join(a) for a in temp.columns.to_flat_index()]\n",
    "    temp = temp.reset_index().drop_duplicates().set_index('Adnum')\n",
    "    temp = temp[~temp.index.duplicated(keep='last')]\n",
    "    print(temp.info())\n",
    "\n",
    "    df_temp = df_temp.drop('idx_1hr',axis=1)\n",
    "    df_temp = df_temp.groupby('Adnum',sort=False).agg([lin_reg_coef])\n",
    "    df_temp.columns = [\"_\".join(a) for a in df_temp.columns.to_flat_index()]\n",
    "    df_temp = df_temp.reset_index().drop_duplicates().set_index('Adnum')\n",
    "    df_temp = df_temp[~df_temp.index.duplicated(keep='last')]\n",
    "\n",
    "    #df = df.merge(temp, right_index=True, left_index=True, how='left')\n",
    "    df = df.merge(df_temp, right_index=True, left_index=True, how='left')\n",
    "    del temp\n",
    "    del df_temp\n",
    "    return df\n",
    "\n",
    "\n",
    "# Calculate features per admission per hour\n",
    "X_TRAIN=x_modelling(memory_upscale(df_train))\n",
    "X_TEST=x_modelling(memory_upscale(df_test))\n",
    "X_VAL=x_modelling(memory_upscale(df_val))\n",
    "print(X_TRAIN.info())\n",
    "print(X_TEST.info())\n",
    "X_TRAIN=X_TRAIN.fillna(value=0)\n",
    "X_VAL=X_VAL.fillna(value=0)\n",
    "X_TEST=X_TEST.fillna(value=0)\n",
    "\n",
    "X_TRAIN=X_TRAIN.append(X_VAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the test and train set for later\n",
    "f = open(os.path.join(output_folder, 'x_train.txt'), 'wb')\n",
    "pickle.dump(X_TRAIN, f)\n",
    "f.close()\n",
    "\n",
    "f = open(os.path.join(output_folder, 'x_test.txt'), 'wb')\n",
    "pickle.dump(X_TEST, f)\n",
    "f.close()\n",
    "\n",
    "f = open(os.path.join(output_folder, 'y_train.txt'), 'wb')\n",
    "pickle.dump(Y_TRAIN, f)\n",
    "f.close()\n",
    "\n",
    "f = open(os.path.join(output_folder, 'y_test.txt'), 'wb')\n",
    "pickle.dump(Y_TEST, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(), n_iter=20,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 14, 18, 22, 26, 30,\n",
       "                                                      34, 38, 42, 46, 50,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Optimizer set up for random forest\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 50, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 20, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_TRAIN, Y_TRAIN)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model Performance\naverage_precision = 0.06159421132252446\nF1 score: 0.0851063829787234 \nAUC = 0.3910867938645717\nModel Performance\naverage_precision = 0.050300422668065725\nF1 score: 0.0 \nAUC = 0.34137673026561915\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix,average_precision_score,f1_score,roc_curve,roc_auc_score,plot_confusion_matrix\n",
    "# Evaluate the best and base model\n",
    "base_model = RandomForestClassifier(n_estimators = 10, random_state = 42,max_depth=10)\n",
    "base_model.fit(X_TRAIN, Y_TRAIN)\n",
    "base_accuracy = evaluate(base_model, X_TEST, Y_TEST,'base_accuracy RF',output_folder)\n",
    "\n",
    "best_random = rf_random.best_estimator_\n",
    "random_accuracy = evaluate(best_random, X_TEST,Y_TEST,'Best_random RF',output_folder)\n",
    "if base_accuracy > random_accuracy:\n",
    "    best_random=base_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Brier score losses: (the smaller the better)\nNo calibration: 0.120\nWith isotonic calibration: 0.097\nWith sigmoid calibration: 0.087\n"
     ]
    }
   ],
   "source": [
    "# Calculate Brier score loss, not used\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from matplotlib import cm\n",
    "\n",
    "# Gaussian Naive-Bayes with no calibration\n",
    "# GaussianNB itself does not support sample-weights\n",
    "prob_pos_clf = best_random.predict_proba(X_TEST)[:, 1]\n",
    "\n",
    "# Gaussian Naive-Bayes with isotonic calibration\n",
    "clf_isotonic = CalibratedClassifierCV(best_random, cv=2, method='isotonic')\n",
    "clf_isotonic.fit(X_TRAIN, Y_TRAIN)\n",
    "prob_pos_isotonic = clf_isotonic.predict_proba(X_TEST)[:, 1]\n",
    "\n",
    "# Gaussian Naive-Bayes with sigmoid calibration\n",
    "clf_sigmoid = CalibratedClassifierCV(best_random, cv=2, method='sigmoid')\n",
    "clf_sigmoid.fit(X_TRAIN, Y_TRAIN)\n",
    "prob_pos_sigmoid = clf_sigmoid.predict_proba(X_TEST)[:, 1]\n",
    "\n",
    "print(\"Brier score losses: (the smaller the better)\")\n",
    "\n",
    "clf_score = brier_score_loss(Y_TEST, prob_pos_clf)\n",
    "print(\"No calibration: %1.3f\" % clf_score)\n",
    "\n",
    "clf_isotonic_score = brier_score_loss(Y_TEST, prob_pos_isotonic)\n",
    "print(\"With isotonic calibration: %1.3f\" % clf_isotonic_score)\n",
    "\n",
    "clf_sigmoid_score = brier_score_loss(Y_TEST, prob_pos_sigmoid)\n",
    "print(\"With sigmoid calibration: %1.3f\" % clf_sigmoid_score)\n",
    "\n",
    "with open(os.path.join(output_folder,f\"Result_scores_all.txt\"),'a') as file:\n",
    "    file.write(f\"Results for RF on training set\\n\\n\")\n",
    "    file.write(\"Brier score losses: (the smaller the better)\\n\")\n",
    "    file.write(\"No calibration: %1.3f\" % clf_score)\n",
    "    file.write(\"With isotonic calibration: %1.3f\" % clf_isotonic_score)\n",
    "    file.write(\"With sigmoid calibration: %1.3f\" % clf_sigmoid_score)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(0.2889637409070993, 0.49320984682204405)\n"
     ]
    }
   ],
   "source": [
    "# Create AUROC plot, confusion matrix and other results for Random Forest\n",
    "\n",
    "from all_own_funct import roc_auc_ci\n",
    "try:\n",
    "    pdf = PdfPages(os.path.join(output_folder,f\"Figures_alls.pdf\"))\n",
    "except PermissionError:\n",
    "    os.remove(os.path.join(output_folder,f\"Figures_all.pdf\"))\n",
    "    os.remove(os.path.join(output_folder,f\"Result_scores_all.txt\"))\n",
    "clf = best_random \n",
    "\n",
    "\n",
    "y_pred_clas=clf.predict(X_TEST)\n",
    "# Predict the probabilities, function depends on used classifier\n",
    "\n",
    "try:\n",
    "    y_pred_prob=clf.predict_proba(X_TEST)\n",
    "    y_pred_prob=y_pred_prob[:,1]\n",
    "except:\n",
    "    try:\n",
    "        y_pred_prob=clf.decision_function(X_TEST)\n",
    "    except:\n",
    "        y_pred_prob=y_pred_clas\n",
    "\n",
    "report=classification_report(Y_TEST,y_pred_clas,target_names=['No Reintubation','Reintubation'])\n",
    "score=clf.score(X_TEST,Y_TEST)\n",
    "average_precision = average_precision_score(Y_TEST, y_pred_prob)\n",
    "f1_s=f1_score(Y_TEST, y_pred_clas)\n",
    "\n",
    "# write scoring metrics to file\n",
    "with open(os.path.join(output_folder,f\"Result_scores_all.txt\"),'a') as file:\n",
    "    file.write(f\"Results for RF on training set\\n\\n\")\n",
    "    file.write(f\"Classification report \\n {report} \\n\")\n",
    "    file.write(f\"Hold_out_scores {score} \\n\")\n",
    "    file.write(f\"Average precision score {average_precision} \\n\")\n",
    "    file.write(f\"F1 score {f1_s} \\n\\n\\n\")\n",
    "\n",
    "plot_confusion_matrix(clf,X_TEST,Y_TEST)\n",
    "plt.title(f\"Confusion matrix of random forrest\")\n",
    "fig=plt.gcf()\n",
    "pdf.savefig(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(Y_TEST,  y_pred_prob)\n",
    "auc = roc_auc_score(Y_TEST, y_pred_prob)\n",
    "\n",
    "n_bootstraps = 2000\n",
    "rng_seed = 42  # control reproducibility\n",
    "bootstrapped_scores = []\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = rng.randint(0, len(y_pred_prob), len(y_pred_prob))\n",
    "    if len(np.unique(Y_TEST[indices])) < 2:\n",
    "        # We need at least one positive and one negative sample for ROC AUC\n",
    "        # to be defined: reject the sample\n",
    "        continue\n",
    "\n",
    "    score = roc_auc_score(Y_TEST[indices], y_pred_prob[indices])\n",
    "    bootstrapped_scores.append(score)\n",
    "    #print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))\n",
    "\n",
    "sorted_scores = np.array(bootstrapped_scores)\n",
    "sorted_scores.sort()\n",
    "\n",
    "# Computing the lower and upper bound of the 90% confidence interval\n",
    "# You can change the bounds percentiles to 0.025 and 0.975 to get\n",
    "# a 95% confidence interval instead.\n",
    "confidence_lower = sorted_scores[int(0.05 * len(sorted_scores))]\n",
    "confidence_upper = sorted_scores[int(0.95 * len(sorted_scores))]\n",
    "with open(os.path.join(output_folder,f\"Result_scores_all.txt\"),'a') as file:\n",
    "    file.write(\"\\nConfidence interval for the score: [{:0.3f} - {:0.3}]\\n\".format(\n",
    "    confidence_lower, confidence_upper))\n",
    "a=roc_auc_ci(Y_TEST,y_pred_prob)\n",
    "print(a)\n",
    "\n",
    "\n",
    "plt.plot(fpr,tpr,label=f\"auc={auc}\",linewidth=1.5,markersize=1)\n",
    "\n",
    "plt.legend(loc=4,fontsize='xx-small')\n",
    "plt.title(f'ROC of Random Forrest hour data')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,1])\n",
    "axes.set_ylim([0,1])\n",
    "fig=plt.gcf()\n",
    "pdf.savefig(fig)\n",
    "plt.close(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  60 | elapsed:    8.4s remaining:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   22.8s finished\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=LogisticRegression(max_iter=1000), n_iter=20,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'C': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n",
       "                                        'class_weight': [{0: 0.1, 1: 0.9},\n",
       "                                                         'balanced'],\n",
       "                                        'penalty': ['l1', 'l2', 'elasticnet'],\n",
       "                                        'solver': ['newton-cg', 'lbfgs',\n",
       "                                                   'liblinear', 'sag',\n",
       "                                                   'saga']},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Hyperparameters for Logistic Regression\n",
    "# The solvers\n",
    "solver =['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "# Class weight optimizer\n",
    "class_weight=[{0:0.1,1:0.9},'balanced',None]\n",
    "# Penalty\n",
    "penalty=['l1', 'l2', 'elasticnet', None]\n",
    "# Inverse of regularization strength\n",
    "C=np.logspace(-3,3,7)\n",
    "# Bootsrapping\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'solver':solver,\n",
    "                'class_weight':class_weight,\n",
    "                'penalty':penalty,\n",
    "                'C':C,\n",
    "                'penalty':penalty}\n",
    "\n",
    "# First create the base model to tune\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "lr_random = RandomizedSearchCV(estimator = lr , param_distributions = random_grid, n_iter = 20, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "lr_random.fit(X_TRAIN, Y_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model Performance\n",
      "average_precision = 0.12384697904612935\n",
      "F1 score: 0.1523809523809524 \n",
      "AUC = 0.684156378600823\n",
      "Model Performance\n",
      "average_precision = 0.10820045734044251\n",
      "F1 score: 0.0 \n",
      "AUC = 0.680976430976431\n",
      "Brier score losses: (the smaller the better)\n",
      "No calibration: 0.167\n",
      "With isotonic calibration: 0.059\n",
      "With sigmoid calibration: 0.059\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the base and optimized model\n",
    "base_model_lr = LogisticRegression(max_iter=1000)\n",
    "base_model_lr.fit(X_TRAIN, Y_TRAIN)\n",
    "base_accuracy_lr = evaluate(base_model_lr, X_TEST, Y_TEST,'base LR',output_folder)\n",
    "\n",
    "best_random_lr = lr_random.best_estimator_\n",
    "random_accuracy_lr = evaluate(best_random_lr, X_TEST,Y_TEST,'Best Random LR',output_folder)\n",
    "\n",
    "if base_accuracy_lr > random_accuracy_lr:\n",
    "    best_random_lr=base_model_lr\n",
    "\n",
    "# Brier scores\n",
    "\"\"\"\n",
    "# Gaussian Naive-Bayes with no calibration\n",
    "# GaussianNB itself does not support sample-weights\n",
    "prob_pos_clf = best_random_lr.predict_proba(X_TEST)[:, 1]\n",
    "\n",
    "# Gaussian Naive-Bayes with isotonic calibration\n",
    "clf_isotonic = CalibratedClassifierCV(best_random_lr, cv=2, method='isotonic')\n",
    "clf_isotonic.fit(X_TRAIN, Y_TRAIN)\n",
    "prob_pos_isotonic = clf_isotonic.predict_proba(X_TEST)[:, 1]\n",
    "\n",
    "# Gaussian Naive-Bayes with sigmoid calibration\n",
    "clf_sigmoid = CalibratedClassifierCV(best_random_lr, cv=2, method='sigmoid')\n",
    "clf_sigmoid.fit(X_TRAIN, Y_TRAIN)\n",
    "prob_pos_sigmoid = clf_sigmoid.predict_proba(X_TEST)[:, 1]\n",
    "\n",
    "print(\"Brier score losses: (the smaller the better)\")\n",
    "\n",
    "clf_score = brier_score_loss(Y_TEST, prob_pos_clf)\n",
    "print(\"No calibration: %1.3f\" % clf_score)\n",
    "\n",
    "clf_isotonic_score = brier_score_loss(Y_TEST, prob_pos_isotonic)\n",
    "print(\"With isotonic calibration: %1.3f\" % clf_isotonic_score)\n",
    "\n",
    "clf_sigmoid_score = brier_score_loss(Y_TEST, prob_pos_sigmoid)\n",
    "print(\"With sigmoid calibration: %1.3f\" % clf_sigmoid_score)\n",
    "\n",
    "with open(os.path.join(output_folder,f\"Result_scores_all.txt\"),'a') as file:\n",
    "    file.write(f\"Results for LR on training set\\n\\n\")\n",
    "    file.write(\"Brier score losses: (the smaller the better)\\n\")\n",
    "    file.write(\"No calibration: %1.3f\" % clf_score)\n",
    "    file.write(\"With isotonic calibration: %1.3f\" % clf_isotonic_score)\n",
    "    file.write(\"With sigmoid calibration: %1.3f\" % clf_sigmoid_score)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(0.5700501130413288, 0.7982626441603172)\n"
     ]
    }
   ],
   "source": [
    "# Create AUROC plot, confusion matrix and other results for logistic regression\n",
    "\n",
    "clf = best_random_lr\n",
    "\n",
    "\n",
    "y_pred_clas=clf.predict(X_TEST)\n",
    "# Predict the probabilities, function depends on used classifier\n",
    "\n",
    "try:\n",
    "    y_pred_prob=clf.predict_proba(X_TEST)\n",
    "    y_pred_prob=y_pred_prob[:,1]\n",
    "except:\n",
    "    try:\n",
    "        y_pred_prob=clf.decision_function(X_TEST)\n",
    "    except:\n",
    "        y_pred_prob=y_pred_clas\n",
    "\n",
    "report=classification_report(Y_TEST,y_pred_clas,target_names=['No Reintubation','Reintubation'])\n",
    "score=clf.score(X_TEST,Y_TEST)\n",
    "average_precision = average_precision_score(Y_TEST, y_pred_prob)\n",
    "f1_s=f1_score(Y_TEST, y_pred_clas)\n",
    "\n",
    "with open(os.path.join(output_folder,f\"Result_scores_all.txt\"),'a') as file:\n",
    "    file.write(f\"Results for LR on training\\n\\n\")\n",
    "    file.write(f\"Classification report \\n {report} \\n\")\n",
    "    file.write(f\"Hold_out_scores {score} \\n\")\n",
    "    file.write(f\"Average precision score {average_precision} \\n\")\n",
    "    file.write(f\"F1 score {f1_s} \\n\\n\\n\")\n",
    "\n",
    "plot_confusion_matrix(clf,X_TEST,Y_TEST)\n",
    "plt.title(f\"Confusion matrix of logistic regression\")\n",
    "fig=plt.gcf()\n",
    "pdf.savefig(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(Y_TEST,  y_pred_prob)\n",
    "auc = roc_auc_score(Y_TEST, y_pred_prob)\n",
    "\n",
    "\n",
    "n_bootstraps = 2000\n",
    "rng_seed = 42  # control reproducibility\n",
    "bootstrapped_scores = []\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = rng.randint(0, len(y_pred_prob), len(y_pred_prob))\n",
    "    if len(np.unique(Y_TEST[indices])) < 2:\n",
    "        # We need at least one positive and one negative sample for ROC AUC\n",
    "        # to be defined: reject the sample\n",
    "        continue\n",
    "\n",
    "    score = roc_auc_score(Y_TEST[indices], y_pred_prob[indices])\n",
    "    bootstrapped_scores.append(score)\n",
    "    #print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))\n",
    "\n",
    "sorted_scores = np.array(bootstrapped_scores)\n",
    "sorted_scores.sort()\n",
    "\n",
    "# Computing the lower and upper bound of the 90% confidence interval\n",
    "# You can change the bounds percentiles to 0.025 and 0.975 to get\n",
    "# a 95% confidence interval instead.\n",
    "confidence_lower = sorted_scores[int(0.05 * len(sorted_scores))]\n",
    "confidence_upper = sorted_scores[int(0.95 * len(sorted_scores))]\n",
    "with open(os.path.join(output_folder,f\"Result_scores_all.txt\"),'a') as file:\n",
    "    file.write(\"\\nConfidence interval for the score: [{:0.3f} - {:0.3}]\\n\".format(\n",
    "    confidence_lower, confidence_upper))\n",
    "\n",
    "a=roc_auc_ci(Y_TEST,y_pred_prob)\n",
    "print(a)\n",
    "\n",
    "plt.plot(fpr,tpr,label=f\"auc={auc}\",linewidth=1.5,markersize=1)\n",
    "\n",
    "\n",
    "plt.legend(loc=4,fontsize='xx-small')\n",
    "plt.title(f'ROC of Logistic Regression data')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,1])\n",
    "axes.set_ylim([0,1])\n",
    "fig=plt.gcf()\n",
    "pdf.savefig(fig)\n",
    "plt.close(fig)\n",
    "pdf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best models\n",
    "import pickle\n",
    "f = open(os.path.join(output_folder,'ran_for.sav'), 'wb')\n",
    "pickle.dump(best_random, f)\n",
    "f.close()\n",
    "\n",
    "f = open(os.path.join(output_folder,'log_reg.sav'), 'wb')\n",
    "pickle.dump(best_random_lr, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}