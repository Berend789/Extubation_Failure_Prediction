{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "3b7e431a861a72513f361682db481bb043ab213b57189a56d42fe6b32fa57c58"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\berend\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\nC:\\Users\\berend\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\nC:\\Users\\berend\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\nC:\\Users\\berend\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import locale\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from all_stand_var import conv_dict, lab_cols, used_cols\n",
    "from all_own_funct import cnfl, value_filtering,y_modelling,x_modelling,evaluate,lin_reg_coef,split_stratified_into_train_val_test\n",
    "from seaborn import heatmap\n",
    "from sklearn.metrics import roc_curve, accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix,average_precision_score,f1_score,roc_curve,roc_auc_score,plot_confusion_matrix\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "locale.setlocale(locale.LC_ALL, 'fr_FR')\n",
    "\n",
    "\n",
    "# output folder in which al results are saved\n",
    "output_folder = os.path.join(os.getcwd(), 'Results_LR_RF_v2','1u_Results_no_mis')\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create processed dataframe, if not already run LR_build.py\n",
    "\"\"\"\n",
    "used_cols = ['pat_hosp_id', 'pat_bd', 'pat_datetime', 'AdmissionDate', 'DischargeDate', 'mon_rr', 'mon_hr', 'mon_sat',\n",
    "              'mon_etco2', 'vent_m_ppeak','vent_m_peep','vent_m_fio2',\n",
    "             'mon_ibp_mean', 'pat_weight_act', 'vent_m_rr', 'vent_m_tv_exp']\n",
    "dtype_dict={'vent_cat': 'category','vent_machine':'category','vent_mode':'category'}\n",
    "df_raw=pd.read_csv(r'data\\sorted_bron_date.csv',delimiter=';',converters=conv_dict,usecols=used_cols,dtype=dtype_dict,parse_dates=['pat_bd','pat_datetime','AdmissionDate', 'DischargeDate'],na_values=['NULL','null', 'Null','nUll','nuLl','nulL'],dayfirst=True)\n",
    "#df = pp.data_pp_function(df_raw,path=output_folder,timestep='12H')\n",
    "#df=df.groupby('Admissionnumber',sort=False).fillna(method='ffill').fillna(method='bfill')\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import LR_build as pp\n",
    "importlib.reload(pp)\n",
    "# Load processed dataframe\n",
    "f = open(os.path.join('./Results_LR_RF_v2/', 'processed_df.txt'), 'rb')\n",
    "df = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#informative plots and descriptive information\n",
    "\"\"\"\n",
    "used_cols = [ 'pat_datetime',  'mon_rr', 'mon_hr', 'mon_sat',\n",
    "              'mon_etco2', 'vent_m_fio2', 'vent_m_ppeak','vent_m_peep',\n",
    "             'mon_ibp_mean','pat_weight_act','vent_m_rr', 'vent_m_tv_exp','Age']\n",
    "for col in used_cols:\n",
    "    print(df[col].describe())\n",
    "\n",
    "print(df[df.index.get_level_values('pat_hosp_id') == 5407]['Reintub'].value_counts())\n",
    "print(df[df.index.get_level_values('pat_hosp_id') == 5150574]['pat_datetime'].tail(100))\n",
    "plt.plot(np.linspace(start=0,stop=720,num=720),df[df.index.get_level_values('pat_hosp_id') == 5407]['vent_m_peep'],'b-')\n",
    "plt.plot(np.linspace(start=0,stop=720,num=720),df[df.index.get_level_values('pat_hosp_id')==5150574]['vent_m_tv_exp'],'r-')\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate the hour intervals, and also the missingness parameter\n",
    "df=df.reset_index(drop=False)\n",
    "df.info()\n",
    "def index_1hr(group):\n",
    "    group['tim']=pd.date_range(start='1/1/2018', periods=len(group), freq='1min')\n",
    "    grouped = pd.Grouper(key='tim', freq='1H')\n",
    "    group['idx_1hr']=group.groupby(grouped,sort=False).ngroup().add(1)\n",
    "    group['mis']=(group['pat_datetime'].max()-group['pat_datetime'].min()).total_seconds()/60\n",
    "    del group['tim']\n",
    "    return group\n",
    "\n",
    "df=df.groupby('Adnum',sort=False,as_index=False).apply(index_1hr)\n",
    "print((df['Adnum'].value_counts()))\n",
    "print(df.info())\n",
    "df['idx_1hr']=df['idx_1hr'].astype(str)\n",
    "print(df['idx_1hr'].unique())\n",
    "df[['mis']]=StandardScaler().fit_transform(df[['mis']])\n",
    "df=df.set_index(['pat_hosp_id','AdmissionDate','DischargeDate'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified spit of dataframe into train, test and validation\n",
    "pat=pd.read_excel(r'Results\\admissiondate_v2.xlsx', parse_dates=[\n",
    "                            'AdmissionDate', 'DischargeDate'])\n",
    "group=pat.groupby('pat_hosp_id',sort=False).max().reset_index()\n",
    "group.drop_duplicates('pat_hosp_id',inplace=True)\n",
    "\n",
    "df_train,df_val,df_test=split_stratified_into_train_val_test(group, stratify_colname='Reintub',\n",
    "                                         frac_train=0.7, frac_val=0.1, frac_test=0.2,\n",
    "                                         random_state=1)\n",
    "\n",
    "train_pat=df_train['pat_hosp_id'].unique()\n",
    "test_pat=df_test['pat_hosp_id'].unique()\n",
    "val_pat=df_val['pat_hosp_id'].unique()\n",
    "\n",
    "df_train = df[df.index.get_level_values('pat_hosp_id').isin(train_pat)]\n",
    "df_val = df[df.index.get_level_values('pat_hosp_id').isin(val_pat)]\n",
    "df_test = df[df.index.get_level_values('pat_hosp_id').isin(test_pat)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.info())\n",
    "def y_modelling(df):\n",
    "    y = df[['Reintub', 'Adnum', 'idx_1hr']]\n",
    "    y = y.groupby(['Adnum', 'idx_1hr'], sort=False).agg(['max'])\n",
    "    y.columns = [\"_\".join(a) for a in y.columns.to_flat_index()]\n",
    "    y.reset_index(drop=True, inplace=True, level='idx_1hr')\n",
    "    y = y.reset_index().drop_duplicates().set_index(\n",
    "        ['Adnum'])\n",
    "    y = y[~y.index.duplicated(keep='last')]\n",
    "    y = y.fillna(value=0)\n",
    "    return y\n",
    "# from dataframwe with label per minute, to array with label per admission\n",
    "Y_TRAIN=y_modelling(df_train)\n",
    "Y_TEST=y_modelling(df_test)\n",
    "Y_VAL=y_modelling(df_val)\n",
    "Y_TRAIN=Y_TRAIN['Reintub_max'].to_numpy()\n",
    "Y_TEST=Y_TEST['Reintub_max'].to_numpy()\n",
    "Y_VAL=Y_VAL['Reintub_max'].to_numpy()\n",
    "Y_TRAIN=np.append(Y_TRAIN,Y_VAL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_modelling(df):\n",
    "    temp = df[['Age', 'Adnum', 'idx_1hr','pat_weight_act']]\n",
    "    df = df.drop(['Age','pat_weight_act','Extubation_date','level_0','pat_datetime', 'Reintub'], axis=1)\n",
    "    df = df.groupby(['Adnum', 'idx_1hr'], sort=False).agg(['mean', 'std',lin_reg_coef])\n",
    "    df.columns = [\"_\".join(a) for a in df.columns.to_flat_index()]\n",
    "    df = df.stack().unstack([2, 1])\n",
    "    df.columns = [\"_\".join(a) for a in df.columns.to_flat_index()]\n",
    "    df = df.reset_index().drop_duplicates().set_index('Adnum')\n",
    "    df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "    temp = temp.groupby('Adnum', sort=False).agg(['mean'])\n",
    "    temp.columns = [\"_\".join(a) for a in temp.columns.to_flat_index()]\n",
    "    temp = temp.reset_index().drop_duplicates().set_index('Adnum')\n",
    "    temp = temp[~temp.index.duplicated(keep='last')]\n",
    "    \"\"\"\n",
    "    df_temp = df_temp.drop('idx_1hr',axis=1)\n",
    "    df_temp = df_temp.groupby('Adnum',sort=False).agg([lin_reg_coef])\n",
    "    df_temp.columns = [\"_\".join(a) for a in df_temp.columns.to_flat_index()]\n",
    "    df_temp = df_temp.reset_index().drop_duplicates().set_index('Adnum')\n",
    "    df_temp = df_temp[~df_temp.index.duplicated(keep='last')]\n",
    "    \"\"\"\n",
    "    df = df.merge(temp, right_index=True, left_index=True, how='left')\n",
    "    #df = df.merge(df_temp, right_index=True, left_index=True, how='left')\n",
    "    del temp\n",
    "    return df\n",
    "\n",
    "# Calculate features per admission\n",
    "X_TRAIN=x_modelling(df_train)\n",
    "X_TEST=x_modelling(df_test)\n",
    "X_VAL=x_modelling(df_val)\n",
    "print(X_TRAIN.info())\n",
    "print(X_TEST.info())\n",
    "X_TRAIN=X_TRAIN.fillna(value=0)\n",
    "X_VAL=X_VAL.fillna(value=0)\n",
    "X_TEST=X_TEST.fillna(value=0)\n",
    "\n",
    "\n",
    "X_TRAIN=X_TRAIN.append(X_VAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Train and test data to file\n",
    "f = open(os.path.join(output_folder, 'x_train.txt'), 'wb')\n",
    "pickle.dump(X_TRAIN, f)\n",
    "f.close()\n",
    "\n",
    "f = open(os.path.join(output_folder, 'x_test.txt'), 'wb')\n",
    "pickle.dump(X_TEST, f)\n",
    "f.close()\n",
    "\n",
    "f = open(os.path.join(output_folder, 'y_train.txt'), 'wb')\n",
    "pickle.dump(Y_TRAIN, f)\n",
    "f.close()\n",
    "\n",
    "f = open(os.path.join(output_folder, 'y_test.txt'), 'wb')\n",
    "pickle.dump(Y_TEST, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomised search for random forest mode\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 50, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 20, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_TRAIN, Y_TRAIN)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the base model vs the optimized model\n",
    "\n",
    "base_model = RandomForestClassifier(n_estimators = 10, random_state = 42,max_depth=10)\n",
    "base_model.fit(X_TRAIN, Y_TRAIN)\n",
    "base_accuracy = evaluate(base_model, X_TEST, Y_TEST,'base_accuracy RF',output_folder)\n",
    "\n",
    "best_random = rf_random.best_estimator_\n",
    "random_accuracy = evaluate(best_random, X_TEST,Y_TEST,'Best_random RF',output_folder)\n",
    "\n",
    "if base_accuracy > random_accuracy:\n",
    "    best_random = base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AUROC plot, confusion matrix and other results\n",
    "\n",
    "from all_own_funct import roc_auc_ci\n",
    "try:\n",
    "    pdf = PdfPages(os.path.join(output_folder,f\"Figures_all_.pdf\"))\n",
    "except PermissionError:\n",
    "    os.remove(os.path.join(output_folder,f\"Figures_all.pdf\"))\n",
    "    os.remove(os.path.join(output_folder,f\"Result_scores_all.txt\"))\n",
    "clf = best_random\n",
    "y_pred_clas=clf.predict(X_TEST)\n",
    "\n",
    "# Predict the probabilities, function depends on used classifier\n",
    "try:\n",
    "    y_pred_prob=clf.predict_proba(X_TEST)\n",
    "    y_pred_prob=y_pred_prob[:,1]\n",
    "except:\n",
    "    try:\n",
    "        y_pred_prob=clf.decision_function(X_TEST)\n",
    "    except:\n",
    "        y_pred_prob=y_pred_clas\n",
    "\n",
    "report=classification_report(Y_TEST,y_pred_clas,target_names=['No Reintubation','Reintubation'])\n",
    "score=clf.score(X_TEST,Y_TEST)\n",
    "average_precision = average_precision_score(Y_TEST, y_pred_prob)\n",
    "f1_s=f1_score(Y_TEST, y_pred_clas)\n",
    "\n",
    "# write scoring metrics to file\n",
    "with open(os.path.join(output_folder,f\"Result_scores_all.txt\"),'a') as file:\n",
    "    file.write(f\"Results for RF on training set\\n\\n\")\n",
    "    file.write(f\"Classification report \\n {report} \\n\")\n",
    "    file.write(f\"Hold_out_scores {score} \\n\")\n",
    "    file.write(f\"Average precision score {average_precision} \\n\")\n",
    "    file.write(f\"F1 score {f1_s} \\n\\n\\n\")\n",
    "\n",
    "plot_confusion_matrix(clf,X_TEST,Y_TEST)\n",
    "plt.title(f\"Confusion matrix of random forrest\")\n",
    "fig=plt.gcf()\n",
    "pdf.savefig(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(Y_TEST,  y_pred_prob)\n",
    "auc = roc_auc_score(Y_TEST, y_pred_prob)\n",
    "\n",
    "\n",
    "# 95%CI calculation using bootstrapping\n",
    "n_bootstraps = 2000\n",
    "rng_seed = 42  # control reproducibility\n",
    "bootstrapped_scores = []\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = rng.randint(0, len(y_pred_prob), len(y_pred_prob))\n",
    "    if len(np.unique(Y_TEST[indices])) < 2:\n",
    "        # We need at least one positive and one negative sample for ROC AUC\n",
    "        # to be defined: reject the sample\n",
    "        continue\n",
    "\n",
    "    score = roc_auc_score(Y_TEST[indices], y_pred_prob[indices])\n",
    "    bootstrapped_scores.append(score)\n",
    "    #print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))\n",
    "\n",
    "sorted_scores = np.array(bootstrapped_scores)\n",
    "sorted_scores.sort()\n",
    "\n",
    "# Computing the lower and upper bound of the 90% confidence interval\n",
    "# You can change the bounds percentiles to 0.025 and 0.975 to get\n",
    "# a 95% confidence interval instead.\n",
    "confidence_lower = sorted_scores[int(0.05 * len(sorted_scores))]\n",
    "confidence_upper = sorted_scores[int(0.95 * len(sorted_scores))]\n",
    "with open(os.path.join(output_folder,f\"Result_scores_all.txt\"),'a') as file:\n",
    "    file.write(\"\\nConfidence interval for the score: [{:0.3f} - {:0.3}]\\n\".format(\n",
    "    confidence_lower, confidence_upper))\n",
    "a=roc_auc_ci(Y_TEST,y_pred_prob)\n",
    "print(a)\n",
    "\n",
    "\n",
    "plt.plot(fpr,tpr,label=f\"auc={auc}\",linewidth=1.5,markersize=1)\n",
    "\n",
    "plt.legend(loc=4,fontsize='xx-small')\n",
    "plt.title(f'ROC of Random Forrest hour data')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,1])\n",
    "axes.set_ylim([0,1])\n",
    "fig=plt.gcf()\n",
    "pdf.savefig(fig)\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for Logistic Regression\n",
    "# The solvers\n",
    "solver =['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "# Class weight optimizer\n",
    "class_weight=[{0:0.1,1:0.9},'balanced',None]\n",
    "# Penalty\n",
    "penalty=['l1', 'l2', 'elasticnet', None]\n",
    "# Inverse of regularization strength\n",
    "C=np.logspace(-3,3,7)\n",
    "# Bootsrapping\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'solver':solver,\n",
    "                'class_weight':class_weight,\n",
    "                'penalty':penalty,\n",
    "                'C':C,\n",
    "                'penalty':penalty}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the base model to tune\n",
    "lr = LogisticRegression(max_iter=500)\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 60 different combinations, and use all available cores\n",
    "lr_random = RandomizedSearchCV(estimator = lr , param_distributions = random_grid, n_iter = 20, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "lr_random.fit(X_TRAIN, Y_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models based on AUROC\n",
    "base_model_lr = LogisticRegression(max_iter=500)\n",
    "base_model_lr.fit(X_TRAIN, Y_TRAIN)\n",
    "base_accuracy_lr = evaluate(base_model, X_TEST, Y_TEST,'base_model LR',output_folder)\n",
    "\n",
    "best_random_lr = lr_random.best_estimator_\n",
    "random_accuracy_lr = evaluate(best_random, X_TEST,Y_TEST,'Best random LR',output_folder)\n",
    "\n",
    "if base_accuracy_lr > random_accuracy_lr:\n",
    "    best_random_lr = base_model_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create AUROC plot, confusion matrix and other results for Logistic regression\n",
    "\n",
    "clf = best_random_lr\n",
    "\n",
    "y_pred_clas=clf.predict(X_TEST)\n",
    "# Predict the probabilities, function depends on used classifier\n",
    "\n",
    "try:\n",
    "    y_pred_prob=clf.predict_proba(X_TEST)\n",
    "    y_pred_prob=y_pred_prob[:,1]\n",
    "except:\n",
    "    try:\n",
    "        y_pred_prob=clf.decision_function(X_TEST)\n",
    "    except:\n",
    "        y_pred_prob=y_pred_clas\n",
    "\n",
    "report=classification_report(Y_TEST,y_pred_clas,target_names=['No Reintubation','Reintubation'])\n",
    "score=clf.score(X_TEST,Y_TEST)\n",
    "average_precision = average_precision_score(Y_TEST, y_pred_prob)\n",
    "f1_s=f1_score(Y_TEST, y_pred_clas)\n",
    "\n",
    "with open(os.path.join(output_folder,f\"Result_scores_all.txt\"),'a') as file:\n",
    "    file.write(f\"Results for LR on training\\n\\n\")\n",
    "    file.write(f\"Classification report \\n {report} \\n\")\n",
    "    file.write(f\"Hold_out_scores {score} \\n\")\n",
    "    file.write(f\"Average precision score {average_precision} \\n\")\n",
    "    file.write(f\"F1 score {f1_s} \\n\\n\\n\")\n",
    "\n",
    "plot_confusion_matrix(clf,X_TEST,Y_TEST)\n",
    "plt.title(f\"Confusion matrix of logistic regression\")\n",
    "fig=plt.gcf()\n",
    "pdf.savefig(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(Y_TEST,  y_pred_prob)\n",
    "auc = roc_auc_score(Y_TEST, y_pred_prob)\n",
    "\n",
    "\n",
    "n_bootstraps = 2000\n",
    "rng_seed = 42  # control reproducibility\n",
    "bootstrapped_scores = []\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = rng.randint(0, len(y_pred_prob), len(y_pred_prob))\n",
    "    if len(np.unique(Y_TEST[indices])) < 2:\n",
    "        # We need at least one positive and one negative sample for ROC AUC\n",
    "        # to be defined: reject the sample\n",
    "        continue\n",
    "\n",
    "    score = roc_auc_score(Y_TEST[indices], y_pred_prob[indices])\n",
    "    bootstrapped_scores.append(score)\n",
    "    #print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))\n",
    "\n",
    "sorted_scores = np.array(bootstrapped_scores)\n",
    "sorted_scores.sort()\n",
    "\n",
    "# Computing the lower and upper bound of the 90% confidence interval\n",
    "# You can change the bounds percentiles to 0.025 and 0.975 to get\n",
    "# a 95% confidence interval instead.\n",
    "confidence_lower = sorted_scores[int(0.05 * len(sorted_scores))]\n",
    "confidence_upper = sorted_scores[int(0.95 * len(sorted_scores))]\n",
    "with open(os.path.join(output_folder,f\"Result_scores_all.txt\"),'a') as file:\n",
    "    file.write(\"\\nConfidence interval for the score: [{:0.3f} - {:0.3}]\\n\".format(\n",
    "    confidence_lower, confidence_upper))\n",
    "\n",
    "a=roc_auc_ci(Y_TEST,y_pred_prob)\n",
    "print(a)\n",
    "\n",
    "plt.plot(fpr,tpr,label=f\"auc={auc}\",linewidth=1.5,markersize=1)\n",
    "\n",
    "\n",
    "plt.legend(loc=4,fontsize='xx-small')\n",
    "plt.title(f'ROC of Logistic Regression data')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,1])\n",
    "axes.set_ylim([0,1])\n",
    "fig=plt.gcf()\n",
    "pdf.savefig(fig)\n",
    "plt.close(fig)\n",
    "pdf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best models\n",
    "import pickle\n",
    "f = open(os.path.join(output_folder,'ran_for.sav'), 'wb')\n",
    "pickle.dump(best_random, f)\n",
    "f.close()\n",
    "\n",
    "f = open(os.path.join(output_folder,'log_reg.sav'), 'wb')\n",
    "pickle.dump(best_random_lr, f)\n",
    "f.close()"
   ]
  }
 ]
}