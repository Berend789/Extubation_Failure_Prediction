{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "3b7e431a861a72513f361682db481bb043ab213b57189a56d42fe6b32fa57c58"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import gc\r\n",
    "from all_stand_var import conv_dict,used_cols\r\n",
    "from all_own_funct import memory_downscale,memory_upscale,evaluate,lin_reg_coef\r\n",
    "import os\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from matplotlib.colors import ListedColormap\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.model_selection import RandomizedSearchCV\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\r\n",
    "from sklearn.metrics import classification_report, confusion_matrix,average_precision_score,f1_score,roc_curve,roc_auc_score,plot_confusion_matrix\r\n",
    "from matplotlib.backends.backend_pdf import PdfPages\r\n",
    "import pickle\r\n",
    "import locale\r\n",
    "import LR_build_CHD as pp\r\n",
    "import datetime as dt\r\n",
    "\r\n",
    "from sklearn.metrics import roc_curve, accuracy_score, roc_auc_score\r\n",
    "from sklearn.metrics import classification_report, confusion_matrix\r\n",
    "\r\n",
    "locale.setlocale(locale.LC_ALL, 'fr_FR')\r\n",
    "\r\n",
    "# Output folder\r\n",
    "output_folder = os.path.join(os.getcwd(), 'Results_LR_RF_CHD_v3','Static_redo')\r\n",
    "if not os.path.exists(output_folder):\r\n",
    "    os.makedirs(output_folder)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tables\r\n",
    "\r\n",
    "# read in data\r\n",
    "df=pd.read_hdf(os.path.join('./Results_CHD_v5/final', 'processed_df.h5'),key='df',mode='r')\r\n",
    "#df=memory_downscale(df)\r\n",
    "print(df.info())\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Descriptive statistics of all used columns\r\n",
    "#used_cols = [ 'pat_datetime',  'mon_rr', 'mon_hr', 'mon_sat',\r\n",
    "#              'mon_etco2', 'vent_m_fio2', 'vent_m_ppeak','vent_m_peep',\r\n",
    "#             'mon_ibp_mean','pat_weight_act','vent_m_rr', 'vent_m_tv_exp','Age']\r\n",
    "#for col in used_cols:\r\n",
    "#    print(df[col].describe())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Figures of different patient trajectories\r\n",
    "\"\"\"\r\n",
    "print(df[df.index.get_level_values('pat_hosp_id') == 8460454]['Reintub'].value_counts())\r\n",
    "print(df[df.index.get_level_values('pat_hosp_id') == 9852266]['pat_datetime'].min())\r\n",
    "print(df[df.index.get_level_values('pat_hosp_id') == 9852266]['pat_datetime'].max())\r\n",
    "plt.plot(np.linspace(start=0,stop=720,num=720),df[df.index.get_level_values('pat_hosp_id') == 3204962]['vent_m_peep'][:720],'r-',label='Failed extubation')\r\n",
    "plt.plot(np.linspace(start=0,stop=720,num=720),df[df.index.get_level_values('pat_hosp_id') == 9852266]['vent_m_peep'],'b-',label='Successful extubation')\r\n",
    "plt.legend(loc=1)\r\n",
    "plt.xlabel('Time (min)')\r\n",
    "plt.ylabel('Scaled value for positive end expiratory pressure (PEEP)')\r\n",
    "plt.tight_layout()\r\n",
    "fig=plt.gcf()\r\n",
    "plt.savefig(os.path.join(output_folder,'peep.png'), format='png',\r\n",
    "             dpi=300, facecolor='white', transparent=True, bbox_inches='tight')\r\n",
    "fig.set_size_inches(12, 8)\r\n",
    "plt.show()\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "plt.plot(np.linspace(start=0,stop=720,num=720),df[df.index.get_level_values('pat_hosp_id') == 1497966]['mon_etco2'],'r-',label='Failed extubation')\r\n",
    "plt.plot(np.linspace(start=0,stop=720,num=720),df[df.index.get_level_values('pat_hosp_id') == 7795892]['mon_etco2'],'b-',label='Successful extubation')\r\n",
    "plt.xlabel('Time (min)')\r\n",
    "plt.ylabel('Scaled value for end-tdail CO2 ')\r\n",
    "plt.legend(loc=1)\r\n",
    "fig=plt.gcf()\r\n",
    "fig.set_size_inches(12, 8)\r\n",
    "plt.savefig(os.path.join(output_folder,'etco2.png'), format='png',\r\n",
    "             dpi=300, facecolor='white', transparent=True, bbox_inches='tight')\r\n",
    "plt.show()\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "plt.plot(np.linspace(start=0,stop=720,num=720),df[df.index.get_level_values('pat_hosp_id') == 5659148]['mon_ibp_mean'],'r-',label='Failed extubation')\r\n",
    "plt.plot(np.linspace(start=0,stop=720,num=720),df[df.index.get_level_values('pat_hosp_id') == 7795892]['mon_ibp_mean'],'b-',label='Successful extubation')\r\n",
    "plt.xlabel('Time (min)')\r\n",
    "plt.ylabel('Scaled value for mean invasive blooppressure')\r\n",
    "plt.legend(loc=1)\r\n",
    "fig=plt.gcf()\r\n",
    "fig.set_size_inches(12, 8)\r\n",
    "plt.savefig(os.path.join(output_folder,'bp.png'), format='png',\r\n",
    "             dpi=300, facecolor='white', transparent=True, bbox_inches='tight')\r\n",
    "\"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import datetime as dt\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "\r\n",
    "df=df.reset_index(drop=False)\r\n",
    "# calculate missingness\r\n",
    "def index_1hr(group):\r\n",
    "    #grouped = pd.Grouper(key='pat_datetime', freq='1H')\r\n",
    "    #group['idx_1hr']=group.groupby(grouped,sort=False).ngroup().add(1)\r\n",
    "    #group['idx_1hr']=group['idx_1hr'].apply(str)\r\n",
    "    group['mis'] = group['pat_datetime'].max()-group['pat_datetime'].min()\r\n",
    "    group['mis'] = group['mis'].dt.seconds.divide(60)\r\n",
    "    return group\r\n",
    "#df.reset_index(drop=True,inplace=True)\r\n",
    "df=df.groupby('Adnum',sort=False,as_index=False).apply(index_1hr)\r\n",
    "# Scale missingness\r\n",
    "df[['mis']]=StandardScaler().fit_transform(df[['mis']])\r\n",
    "df=df.set_index(['pat_hosp_id','OK_datum'])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Stratified split to train, test and validation set\r\n",
    "pat=pd.read_excel(r'Results_CHD\\admissiondate_CHD0.xlsx',\r\n",
    "                    parse_dates=['OK_datum'],index_col=0)\r\n",
    "group=pat.groupby('pat_hosp_id',sort=False).max().reset_index()\r\n",
    "group.drop_duplicates('pat_hosp_id',inplace=True)\r\n",
    "\r\n",
    "df_train,df_val,df_test=pp.split_stratified_into_train_val_test(group, stratify_colname='Reintub',\r\n",
    "                                         frac_train=0.7, frac_val=0.1, frac_test=0.2,\r\n",
    "                                         random_state=1)\r\n",
    "\r\n",
    "train_pat=df_train['pat_hosp_id'].unique()\r\n",
    "test_pat=df_test['pat_hosp_id'].unique()\r\n",
    "val_pat=df_val['pat_hosp_id'].unique()\r\n",
    "\r\n",
    "df_train = df[df.index.get_level_values('pat_hosp_id').isin(train_pat)]\r\n",
    "df_val = df[df.index.get_level_values('pat_hosp_id').isin(val_pat)]\r\n",
    "df_test = df[df.index.get_level_values('pat_hosp_id').isin(test_pat)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from dataframwe with label per minute, to array with label per admission\r\n",
    "def y_modelling(df):\r\n",
    "    y=df[['Reintub','Adnum']]\r\n",
    "    y=y.groupby(['Adnum'],sort=False).agg(['max'])\r\n",
    "    y.columns = [\"_\".join(a) for a in y.columns.to_flat_index()]\r\n",
    "    y=y.reset_index().drop_duplicates().set_index('Adnum')\r\n",
    "    y=y[~y.index.duplicated(keep='last')]\r\n",
    "    return y\r\n",
    "Y_TRAIN=y_modelling(df_train)\r\n",
    "Y_TEST=y_modelling(df_test)\r\n",
    "Y_VAL=y_modelling(df_val)\r\n",
    "Y_TRAIN=Y_TRAIN['Reintub_max'].to_numpy()\r\n",
    "Y_TEST=Y_TEST['Reintub_max'].to_numpy()\r\n",
    "Y_VAL=Y_VAL['Reintub_max'].to_numpy()\r\n",
    "Y_TRAIN=np.append(Y_TRAIN,Y_VAL)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def x_modelling(df):\r\n",
    "    df=df[['Adnum','mis','pat_weight_act','Age','Diagnose']]\r\n",
    "    #df=df.drop(['Extubation_date','level_0','pat_datetime', 'Reintub'],axis=1)\r\n",
    "    df=df.groupby(['Adnum'],sort=False).agg(['mean'])\r\n",
    "    df.columns = [\"_\".join(a) for a in df.columns.to_flat_index()]\r\n",
    "    df=df.reset_index().drop_duplicates().set_index('Adnum')\r\n",
    "    return df\r\n",
    "X_TRAIN=memory_upscale(x_modelling(df_train))\r\n",
    "X_TEST=memory_upscale(x_modelling(df_test))\r\n",
    "X_VAL=memory_upscale(x_modelling(df_val))\r\n",
    "\r\n",
    "X_TRAIN=X_TRAIN.fillna(value=0)\r\n",
    "X_VAL=X_VAL.fillna(value=0)\r\n",
    "X_TEST=X_TEST.fillna(value=0)\r\n",
    "X_TRAIN=X_TRAIN.append(X_VAL)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Save train and test data\r\n",
    "f = open(os.path.join(output_folder, 'x_train.txt'), 'wb')\r\n",
    "pickle.dump(X_TRAIN, f)\r\n",
    "f.close()\r\n",
    "\r\n",
    "f = open(os.path.join(output_folder, 'x_test.txt'), 'wb')\r\n",
    "pickle.dump(X_TEST, f)\r\n",
    "f.close()\r\n",
    "\r\n",
    "f = open(os.path.join(output_folder, 'y_train.txt'), 'wb')\r\n",
    "pickle.dump(Y_TRAIN, f)\r\n",
    "f.close()\r\n",
    "\r\n",
    "f = open(os.path.join(output_folder, 'y_test.txt'), 'wb')\r\n",
    "pickle.dump(Y_TEST, f)\r\n",
    "f.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Optimizer set up for random forest\r\n",
    "# Number of trees in random forest\r\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\r\n",
    "# Number of features to consider at every split\r\n",
    "max_features = ['auto', 'sqrt']\r\n",
    "# Maximum number of levels in tree\r\n",
    "max_depth = [int(x) for x in np.linspace(10, 50, num = 11)]\r\n",
    "max_depth.append(None)\r\n",
    "# Minimum number of samples required to split a node\r\n",
    "min_samples_split = [2, 5, 10]\r\n",
    "# Minimum number of samples required at each leaf node\r\n",
    "min_samples_leaf = [1, 2, 4]\r\n",
    "# Method of selecting samples for training each tree\r\n",
    "bootstrap = [True, False]\r\n",
    "# Create the random grid\r\n",
    "random_grid = {'n_estimators': n_estimators,\r\n",
    "               'max_features': max_features,\r\n",
    "               'max_depth': max_depth,\r\n",
    "               'min_samples_split': min_samples_split,\r\n",
    "               'min_samples_leaf': min_samples_leaf,\r\n",
    "               'bootstrap': bootstrap}\r\n",
    "\r\n",
    "# Use the random grid to search for best hyperparameters\r\n",
    "# First create the base model to tune\r\n",
    "rf = RandomForestClassifier()\r\n",
    "# Random search of parameters, using 3 fold cross validation, \r\n",
    "# search across 100 different combinations, and use all available cores\r\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 20, cv = 3, verbose=2, random_state=42, n_jobs = -1)\r\n",
    "# Fit the random search model\r\n",
    "rf_random.fit(X_TRAIN, Y_TRAIN)\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix,average_precision_score,f1_score,roc_curve,roc_auc_score,plot_confusion_matrix\r\n",
    "\r\n",
    "# Evaluate the best and base model\r\n",
    "base_model = RandomForestClassifier(n_estimators = 10, random_state = 42,max_depth=10)\r\n",
    "base_model.fit(X_TRAIN, Y_TRAIN)\r\n",
    "base_accuracy = evaluate(base_model, X_TEST, Y_TEST,'base_accuracy RF',output_folder)\r\n",
    "\r\n",
    "best_random = rf_random.best_estimator_\r\n",
    "random_accuracy = evaluate(best_random, X_TEST,Y_TEST,'Best_random RF',output_folder)\r\n",
    "if base_accuracy > random_accuracy:\r\n",
    "    best_random=base_model\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import brier_score_loss\r\n",
    "from sklearn.calibration import CalibratedClassifierCV\r\n",
    "from matplotlib import cm\r\n",
    "# calculate Brier score loss, not used\r\n",
    "\"\"\"\r\n",
    "# Gaussian Naive-Bayes with no calibration\r\n",
    "# GaussianNB itself does not support sample-weights\r\n",
    "prob_pos_clf = best_random.predict_proba(X_TEST)[:, 1]\r\n",
    "\r\n",
    "# Gaussian Naive-Bayes with isotonic calibration\r\n",
    "clf_isotonic = CalibratedClassifierCV(best_random, cv=2, method='isotonic')\r\n",
    "clf_isotonic.fit(X_TRAIN, Y_TRAIN)\r\n",
    "prob_pos_isotonic = clf_isotonic.predict_proba(X_TEST)[:, 1]\r\n",
    "\r\n",
    "# Gaussian Naive-Bayes with sigmoid calibration\r\n",
    "clf_sigmoid = CalibratedClassifierCV(best_random, cv=2, method='sigmoid')\r\n",
    "clf_sigmoid.fit(X_TRAIN, Y_TRAIN)\r\n",
    "prob_pos_sigmoid = clf_sigmoid.predict_proba(X_TEST)[:, 1]\r\n",
    "\r\n",
    "print(\"Brier score losses: (the smaller the better)\")\r\n",
    "\r\n",
    "clf_score = brier_score_loss(Y_TEST, prob_pos_clf)\r\n",
    "print(\"No calibration: %1.3f\" % clf_score)\r\n",
    "\r\n",
    "clf_isotonic_score = brier_score_loss(Y_TEST, prob_pos_isotonic)\r\n",
    "print(\"With isotonic calibration: %1.3f\" % clf_isotonic_score)\r\n",
    "\r\n",
    "clf_sigmoid_score = brier_score_loss(Y_TEST, prob_pos_sigmoid)\r\n",
    "print(\"With sigmoid calibration: %1.3f\" % clf_sigmoid_score)\r\n",
    "\r\n",
    "with open(os.path.join(output_folder,f\"Result_scores_all.txt\"),'a') as file:\r\n",
    "    file.write(f\"Results for RF on training set\\n\\n\")\r\n",
    "    file.write(\"Brier score losses: (the smaller the better)\\n\")\r\n",
    "    file.write(\"No calibration: %1.3f\" % clf_score)\r\n",
    "    file.write(\"With isotonic calibration: %1.3f\" % clf_isotonic_score)\r\n",
    "    file.write(\"With sigmoid calibration: %1.3f\" % clf_sigmoid_score)\r\n",
    "\"\"\"\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create AUROC plot, confusion matrix and other results for Random Forest\r\n",
    "\r\n",
    "from all_own_funct import roc_auc_ci\r\n",
    "try:\r\n",
    "    pdf = PdfPages(os.path.join(output_folder,f\"Figures_alls.pdf\"))\r\n",
    "except PermissionError:\r\n",
    "    os.remove(os.path.join(output_folder,f\"Figures_all.pdf\"))\r\n",
    "    os.remove(os.path.join(output_folder,f\"Result_scores_all.txt\"))\r\n",
    "clf = best_random \r\n",
    "\r\n",
    "\r\n",
    "y_pred_clas=clf.predict(X_TEST)\r\n",
    "# Predict the probabilities, function depends on used classifier\r\n",
    "\r\n",
    "try:\r\n",
    "    y_pred_prob=clf.predict_proba(X_TEST)\r\n",
    "    y_pred_prob=y_pred_prob[:,1]\r\n",
    "except:\r\n",
    "    try:\r\n",
    "        y_pred_prob=clf.decision_function(X_TEST)\r\n",
    "    except:\r\n",
    "        y_pred_prob=y_pred_clas\r\n",
    "\r\n",
    "report=classification_report(Y_TEST,y_pred_clas,target_names=['No Reintubation','Reintubation'])\r\n",
    "score=clf.score(X_TEST,Y_TEST)\r\n",
    "average_precision = average_precision_score(Y_TEST, y_pred_prob)\r\n",
    "f1_s=f1_score(Y_TEST, y_pred_clas)\r\n",
    "\r\n",
    "# write scoring metrics to file\r\n",
    "with open(os.path.join(output_folder,f\"Result_scores_all.txt\"),'a') as file:\r\n",
    "    file.write(f\"Results for RF on training set\\n\\n\")\r\n",
    "    file.write(f\"Classification report \\n {report} \\n\")\r\n",
    "    file.write(f\"Hold_out_scores {score} \\n\")\r\n",
    "    file.write(f\"Average precision score {average_precision} \\n\")\r\n",
    "    file.write(f\"F1 score {f1_s} \\n\\n\\n\")\r\n",
    "\r\n",
    "plot_confusion_matrix(clf,X_TEST,Y_TEST)\r\n",
    "plt.title(f\"Confusion matrix of random forrest\")\r\n",
    "fig=plt.gcf()\r\n",
    "pdf.savefig(fig)\r\n",
    "plt.close(fig)\r\n",
    "\r\n",
    "fpr, tpr, _ = roc_curve(Y_TEST,  y_pred_prob)\r\n",
    "auc = roc_auc_score(Y_TEST, y_pred_prob)\r\n",
    "\r\n",
    "n_bootstraps = 2000\r\n",
    "rng_seed = 42  # control reproducibility\r\n",
    "bootstrapped_scores = []\r\n",
    "\r\n",
    "rng = np.random.RandomState(rng_seed)\r\n",
    "for i in range(n_bootstraps):\r\n",
    "    # bootstrap by sampling with replacement on the prediction indices\r\n",
    "    indices = rng.randint(0, len(y_pred_prob), len(y_pred_prob))\r\n",
    "    if len(np.unique(Y_TEST[indices])) < 2:\r\n",
    "        # We need at least one positive and one negative sample for ROC AUC\r\n",
    "        # to be defined: reject the sample\r\n",
    "        continue\r\n",
    "\r\n",
    "    score = roc_auc_score(Y_TEST[indices], y_pred_prob[indices])\r\n",
    "    bootstrapped_scores.append(score)\r\n",
    "    #print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))\r\n",
    "\r\n",
    "sorted_scores = np.array(bootstrapped_scores)\r\n",
    "sorted_scores.sort()\r\n",
    "\r\n",
    "# Computing the lower and upper bound of the 90% confidence interval\r\n",
    "# You can change the bounds percentiles to 0.025 and 0.975 to get\r\n",
    "# a 95% confidence interval instead.\r\n",
    "confidence_lower = sorted_scores[int(0.05 * len(sorted_scores))]\r\n",
    "confidence_upper = sorted_scores[int(0.95 * len(sorted_scores))]\r\n",
    "with open(os.path.join(output_folder,f\"Result_scores_all.txt\"),'a') as file:\r\n",
    "    file.write(\"\\nConfidence interval for the score: [{:0.3f} - {:0.3}]\\n\".format(\r\n",
    "    confidence_lower, confidence_upper))\r\n",
    "a=roc_auc_ci(Y_TEST,y_pred_prob)\r\n",
    "print(a)\r\n",
    "\r\n",
    "\r\n",
    "plt.plot(fpr,tpr,label=f\"auc={auc}\",linewidth=1.5,markersize=1)\r\n",
    "\r\n",
    "plt.legend(loc=4,fontsize='xx-small')\r\n",
    "plt.title(f'ROC of Random Forrest hour data')\r\n",
    "plt.xlabel('False Positive Rate')\r\n",
    "plt.ylabel('True Positive Rate')\r\n",
    "axes = plt.gca()\r\n",
    "axes.set_xlim([0,1])\r\n",
    "axes.set_ylim([0,1])\r\n",
    "fig=plt.gcf()\r\n",
    "pdf.savefig(fig)\r\n",
    "plt.close(fig)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Hyperparameters for Logistic Regression\r\n",
    "# The solvers\r\n",
    "solver =['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\r\n",
    "# Class weight optimizer\r\n",
    "class_weight=[{0:0.1,1:0.9},'balanced',None]\r\n",
    "# Penalty\r\n",
    "penalty=['l1', 'l2', 'elasticnet', None]\r\n",
    "# Inverse of regularization strength\r\n",
    "C=np.logspace(-3,3,7)\r\n",
    "# Bootsrapping\r\n",
    "bootstrap = [True, False]\r\n",
    "# Create the random grid\r\n",
    "random_grid = {'solver':solver,\r\n",
    "                'class_weight':class_weight,\r\n",
    "                'penalty':penalty,\r\n",
    "                'C':C,\r\n",
    "                'penalty':penalty}\r\n",
    "\r\n",
    "# First create the base model to tune\r\n",
    "lr = LogisticRegression(max_iter=1000)\r\n",
    "# Random search of parameters, using 3 fold cross validation, \r\n",
    "# search across 100 different combinations, and use all available cores\r\n",
    "lr_random = RandomizedSearchCV(estimator = lr , param_distributions = random_grid, n_iter = 20, cv = 3, verbose=2, random_state=42, n_jobs = -1)\r\n",
    "# Fit the random search model\r\n",
    "lr_random.fit(X_TRAIN, Y_TRAIN)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Evaluate the base and optimized model\r\n",
    "base_model_lr = LogisticRegression(max_iter=1000)\r\n",
    "base_model_lr.fit(X_TRAIN, Y_TRAIN)\r\n",
    "base_accuracy_lr = evaluate(base_model_lr, X_TEST, Y_TEST,'base LR',output_folder)\r\n",
    "\r\n",
    "best_random_lr = lr_random.best_estimator_\r\n",
    "random_accuracy_lr = evaluate(best_random_lr, X_TEST,Y_TEST,'Best Random LR',output_folder)\r\n",
    "\r\n",
    "if base_accuracy_lr > random_accuracy_lr:\r\n",
    "    best_random_lr=base_model_lr\r\n",
    "\r\n",
    "# Brier scores\r\n",
    "\"\"\"\r\n",
    "# Gaussian Naive-Bayes with no calibration\r\n",
    "# GaussianNB itself does not support sample-weights\r\n",
    "prob_pos_clf = best_random_lr.predict_proba(X_TEST)[:, 1]\r\n",
    "\r\n",
    "# Gaussian Naive-Bayes with isotonic calibration\r\n",
    "clf_isotonic = CalibratedClassifierCV(best_random_lr, cv=2, method='isotonic')\r\n",
    "clf_isotonic.fit(X_TRAIN, Y_TRAIN)\r\n",
    "prob_pos_isotonic = clf_isotonic.predict_proba(X_TEST)[:, 1]\r\n",
    "\r\n",
    "# Gaussian Naive-Bayes with sigmoid calibration\r\n",
    "clf_sigmoid = CalibratedClassifierCV(best_random_lr, cv=2, method='sigmoid')\r\n",
    "clf_sigmoid.fit(X_TRAIN, Y_TRAIN)\r\n",
    "prob_pos_sigmoid = clf_sigmoid.predict_proba(X_TEST)[:, 1]\r\n",
    "\r\n",
    "print(\"Brier score losses: (the smaller the better)\")\r\n",
    "\r\n",
    "clf_score = brier_score_loss(Y_TEST, prob_pos_clf)\r\n",
    "print(\"No calibration: %1.3f\" % clf_score)\r\n",
    "\r\n",
    "clf_isotonic_score = brier_score_loss(Y_TEST, prob_pos_isotonic)\r\n",
    "print(\"With isotonic calibration: %1.3f\" % clf_isotonic_score)\r\n",
    "\r\n",
    "clf_sigmoid_score = brier_score_loss(Y_TEST, prob_pos_sigmoid)\r\n",
    "print(\"With sigmoid calibration: %1.3f\" % clf_sigmoid_score)\r\n",
    "\r\n",
    "with open(os.path.join(output_folder,f\"Result_scores_all.txt\"),'a') as file:\r\n",
    "    file.write(f\"Results for LR on training set\\n\\n\")\r\n",
    "    file.write(\"Brier score losses: (the smaller the better)\\n\")\r\n",
    "    file.write(\"No calibration: %1.3f\" % clf_score)\r\n",
    "    file.write(\"With isotonic calibration: %1.3f\" % clf_isotonic_score)\r\n",
    "    file.write(\"With sigmoid calibration: %1.3f\" % clf_sigmoid_score)\r\n",
    "\"\"\""
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create AUROC plot, confusion matrix and other results for logistic regression\r\n",
    "\r\n",
    "clf = best_random_lr\r\n",
    "\r\n",
    "\r\n",
    "y_pred_clas=clf.predict(X_TEST)\r\n",
    "# Predict the probabilities, function depends on used classifier\r\n",
    "\r\n",
    "try:\r\n",
    "    y_pred_prob=clf.predict_proba(X_TEST)\r\n",
    "    y_pred_prob=y_pred_prob[:,1]\r\n",
    "except:\r\n",
    "    try:\r\n",
    "        y_pred_prob=clf.decision_function(X_TEST)\r\n",
    "    except:\r\n",
    "        y_pred_prob=y_pred_clas\r\n",
    "\r\n",
    "report=classification_report(Y_TEST,y_pred_clas,target_names=['No Reintubation','Reintubation'])\r\n",
    "score=clf.score(X_TEST,Y_TEST)\r\n",
    "average_precision = average_precision_score(Y_TEST, y_pred_prob)\r\n",
    "f1_s=f1_score(Y_TEST, y_pred_clas)\r\n",
    "\r\n",
    "with open(os.path.join(output_folder,f\"Result_scores_all.txt\"),'a') as file:\r\n",
    "    file.write(f\"Results for LR on training\\n\\n\")\r\n",
    "    file.write(f\"Classification report \\n {report} \\n\")\r\n",
    "    file.write(f\"Hold_out_scores {score} \\n\")\r\n",
    "    file.write(f\"Average precision score {average_precision} \\n\")\r\n",
    "    file.write(f\"F1 score {f1_s} \\n\\n\\n\")\r\n",
    "\r\n",
    "plot_confusion_matrix(clf,X_TEST,Y_TEST)\r\n",
    "plt.title(f\"Confusion matrix of logistic regression\")\r\n",
    "fig=plt.gcf()\r\n",
    "pdf.savefig(fig)\r\n",
    "plt.close(fig)\r\n",
    "\r\n",
    "fpr, tpr, _ = roc_curve(Y_TEST,  y_pred_prob)\r\n",
    "auc = roc_auc_score(Y_TEST, y_pred_prob)\r\n",
    "\r\n",
    "\r\n",
    "n_bootstraps = 2000\r\n",
    "rng_seed = 42  # control reproducibility\r\n",
    "bootstrapped_scores = []\r\n",
    "\r\n",
    "rng = np.random.RandomState(rng_seed)\r\n",
    "for i in range(n_bootstraps):\r\n",
    "    # bootstrap by sampling with replacement on the prediction indices\r\n",
    "    indices = rng.randint(0, len(y_pred_prob), len(y_pred_prob))\r\n",
    "    if len(np.unique(Y_TEST[indices])) < 2:\r\n",
    "        # We need at least one positive and one negative sample for ROC AUC\r\n",
    "        # to be defined: reject the sample\r\n",
    "        continue\r\n",
    "\r\n",
    "    score = roc_auc_score(Y_TEST[indices], y_pred_prob[indices])\r\n",
    "    bootstrapped_scores.append(score)\r\n",
    "    #print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))\r\n",
    "\r\n",
    "sorted_scores = np.array(bootstrapped_scores)\r\n",
    "sorted_scores.sort()\r\n",
    "\r\n",
    "# Computing the lower and upper bound of the 90% confidence interval\r\n",
    "# You can change the bounds percentiles to 0.025 and 0.975 to get\r\n",
    "# a 95% confidence interval instead.\r\n",
    "confidence_lower = sorted_scores[int(0.05 * len(sorted_scores))]\r\n",
    "confidence_upper = sorted_scores[int(0.95 * len(sorted_scores))]\r\n",
    "with open(os.path.join(output_folder,f\"Result_scores_all.txt\"),'a') as file:\r\n",
    "    file.write(\"\\nConfidence interval for the score: [{:0.3f} - {:0.3}]\\n\".format(\r\n",
    "    confidence_lower, confidence_upper))\r\n",
    "\r\n",
    "a=roc_auc_ci(Y_TEST,y_pred_prob)\r\n",
    "print(a)\r\n",
    "\r\n",
    "plt.plot(fpr,tpr,label=f\"auc={auc}\",linewidth=1.5,markersize=1)\r\n",
    "\r\n",
    "\r\n",
    "plt.legend(loc=4,fontsize='xx-small')\r\n",
    "plt.title(f'ROC of Logistic Regression data')\r\n",
    "plt.xlabel('False Positive Rate')\r\n",
    "plt.ylabel('True Positive Rate')\r\n",
    "axes = plt.gca()\r\n",
    "axes.set_xlim([0,1])\r\n",
    "axes.set_ylim([0,1])\r\n",
    "fig=plt.gcf()\r\n",
    "pdf.savefig(fig)\r\n",
    "plt.close(fig)\r\n",
    "pdf.close()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Save the best models\r\n",
    "import pickle\r\n",
    "f = open(os.path.join(output_folder,'ran_for.sav'), 'wb')\r\n",
    "pickle.dump(best_random, f)\r\n",
    "f.close()\r\n",
    "\r\n",
    "f = open(os.path.join(output_folder,'log_reg.sav'), 'wb')\r\n",
    "pickle.dump(best_random_lr, f)\r\n",
    "f.close()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}